<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>网站用户行为分析（大数据课设） | Hik🐣</title><meta name="author" content="韩梦婷,3102454836@qq.com"><meta name="copyright" content="韩梦婷"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一. 环境搭建（一）安装Linux系统(ubuntu18.04.6)   安装tools  （二）安装Hadoop 创建hadoop用户如果你安装 Ubuntu 的时候不是用的 “hadoop” 用户，那么需要增加一个名为 hadoop 的用户。 首先按 ctrl+alt+t 打开终端窗口，输入如下命令创建新用户  1sudo useradd -m hadoop -s &#x2F;bin&#x2F;bash 接着使用">
<meta property="og:type" content="article">
<meta property="og:title" content="网站用户行为分析（大数据课设）">
<meta property="og:url" content="https://hanmengting.github.io/2023/10/22/%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%EF%BC%88%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AF%BE%E8%AE%BE%EF%BC%89/index.html">
<meta property="og:site_name" content="Hik🐣">
<meta property="og:description" content="一. 环境搭建（一）安装Linux系统(ubuntu18.04.6)   安装tools  （二）安装Hadoop 创建hadoop用户如果你安装 Ubuntu 的时候不是用的 “hadoop” 用户，那么需要增加一个名为 hadoop 的用户。 首先按 ctrl+alt+t 打开终端窗口，输入如下命令创建新用户  1sudo useradd -m hadoop -s &#x2F;bin&#x2F;bash 接着使用">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://img.netbian.com/file/2024/0310/161609O75fK.jpg">
<meta property="article:published_time" content="2023-10-22T10:05:55.000Z">
<meta property="article:modified_time" content="2024-04-07T08:28:02.508Z">
<meta property="article:author" content="韩梦婷">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://img.netbian.com/file/2024/0310/161609O75fK.jpg"><link rel="shortcut icon" href="https://ts1.cn.mm.bing.net/th/id/R-C.0ca4781494f9fdc7432ba6374bb1962d?rik=WQLMR7K%2beFa%2fig&riu=http%3a%2f%2fimg.touxiangwu.com%2f2020%2f3%2fbYNf22.jpg&ehk=hwyMelCP9403YdsnSLoChvJyKF0qGGFBD1E0DBjbYz4%3d&risl=&pid=ImgRaw&r=0"><link rel="canonical" href="https://hanmengting.github.io/2023/10/22/%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%EF%BC%88%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AF%BE%E8%AE%BE%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '网站用户行为分析（大数据课设）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-07 16:28:02'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://p.qqan.com/up/2020-12/16086065967661528.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('http://img.netbian.com/file/2024/0310/161609O75fK.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Hik🐣"><span class="site-name">Hik🐣</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">网站用户行为分析（大数据课设）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-10-22T10:05:55.000Z" title="发表于 2023-10-22 18:05:55">2023-10-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-07T08:28:02.508Z" title="更新于 2024-04-07 16:28:02">2024-04-07</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">14.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>54分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="网站用户行为分析（大数据课设）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="一-环境搭建"><a href="#一-环境搭建" class="headerlink" title="一. 环境搭建"></a>一. 环境搭建</h2><h3 id="（一）安装Linux系统-ubuntu18-04-6"><a href="#（一）安装Linux系统-ubuntu18-04-6" class="headerlink" title="（一）安装Linux系统(ubuntu18.04.6)"></a>（一）安装Linux系统(ubuntu18.04.6)</h3><p><img src="image-20230517195053166.png" alt="image-20230517195053166"></p>
<p><img src="image-20230517195108748.png" alt="image-20230517195108748"></p>
<p><img src="image-20230517195124488.png" alt="image-20230517195124488"><img src="image-20230517195246210.png" alt="image-20230517195246210"><img src="image-20230517195305286.png" alt="image-20230517195305286"><img src="image-20230517195321219.png" alt="image-20230517195321219"><img src="image-20230517195330297.png" alt="image-20230517195330297"><img src="image-20230517195339997.png" alt="image-20230517195339997"><img src="image-20230517195352004.png" alt="image-20230517195352004"><img src="image-20230517195405998.png" alt="image-20230517195405998"><img src="image-20230517195423410.png" alt="image-20230517195423410"><img src="image-20230517195453308.png" alt="image-20230517195453308"><img src="image-20230517195542174.png" alt="image-20230517195542174"><img src="image-20230517195555376.png" alt="image-20230517195555376"><img src="image-20230517195610805.png" alt="image-20230517195610805"><img src="image-20230517195621697.png" alt="image-20230517195621697"><img src="image-20230517195643898.png" alt="image-20230517195643898"><img src="image-20230517195706163.png" alt="image-20230517195706163"><img src="image-20230517195713515.png" alt="image-20230517195713515"></p>
<p>安装tools</p>
<p><img src="image-20230517210758112.png" alt="image-20230517210758112"></p>
<h3 id="（二）安装Hadoop"><a href="#（二）安装Hadoop" class="headerlink" title="（二）安装Hadoop"></a>（二）安装Hadoop</h3><ol>
<li><h4 id="创建hadoop用户"><a href="#创建hadoop用户" class="headerlink" title="创建hadoop用户"></a>创建hadoop用户</h4><p>如果你安装 Ubuntu 的时候不是用的 “hadoop” 用户，那么需要增加一个名为 hadoop 的用户。</p>
<p>首先按 <strong>ctrl+alt+t</strong> 打开终端窗口，输入如下命令创建新用户 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo useradd -m hadoop -s /bin/bash</span><br></pre></td></tr></table></figure>
<p>接着使用如下命令设置密码，可简单设置为 hadoop，按提示输入两次密码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo passwd hadoop</span><br></pre></td></tr></table></figure>
<p>可为 hadoop 用户增加管理员权限，方便部署，避免一些对新手来说比较棘手的权限问题</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo adduser hadoop sudo</span><br></pre></td></tr></table></figure>
<p><img src="image-20230517210933668.png" alt="image-20230517210933668" style="zoom:80%;" /></p>
</li>
<li><h4 id="更新apt"><a href="#更新apt" class="headerlink" title="更新apt"></a>更新apt</h4><p>用 hadoop 用户登录后,我们先更新一下 apt，后续我们使用 apt 安装软件，如果没更新可能有一些软件安装不了。按 ctrl+alt+t 打开终端窗口，执行如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure>
<p><img src="image-20230517211837447.png" alt="image-20230517211837447"></p>
<p>安装一下vim</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install vim</span><br></pre></td></tr></table></figure>
<p><img src="image-20230517213609371.png" alt="image-20230517213609371"></p>
</li>
<li><h4 id="安装ssh、配置ssh无密码登录"><a href="#安装ssh、配置ssh无密码登录" class="headerlink" title="安装ssh、配置ssh无密码登录"></a>安装ssh、配置ssh无密码登录</h4><p>集群、单节点模式都需要用到 SSH 登陆（类似于远程登陆，你可以登录某台 Linux 主机，并且在上面运行命令），Ubuntu 默认已安装了 SSH client，此外还需要安装 SSH server：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure>
<p><img src="image-20230518083554707.png" alt="image-20230518083554707"></p>
<p>安装后，可以使用如下命令登陆本机：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost</span><br></pre></td></tr></table></figure>
<p><img src="image-20230518083623870.png" alt="image-20230518083623870"></p>
<p>但这样登陆是需要每次输入密码的，我们需要配置成SSH无密码登陆比较方便。</p>
<p>首先退出刚才的 ssh，就回到了我们原先的终端窗口，然后利用 ssh-keygen 生成密钥，并将密钥加入到授权中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">exit      # 退出刚才的 ssh localhost</span><br><span class="line">cd ~/.ssh/   #若没有该目录请先执行一次ssh localhost</span><br><span class="line">ssh-keygen -t rsa     # 会有提示，都按回车就可以</span><br><span class="line">cat ./id_rsa.pub &gt;&gt; ./authorized_keys  # 加入授权</span><br></pre></td></tr></table></figure>
<p>现在就可以直接登录了</p>
<p><img src="image-20230518175015962.png" alt="image-20230518175015962"></p>
</li>
<li><h4 id="安装Java环境"><a href="#安装Java环境" class="headerlink" title="安装Java环境"></a>安装Java环境</h4><p>在Linux命令行界面中，执行如下Shell命令（注意：当前登录用户名是hadoop）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/lib</span><br><span class="line">2.sudo mkdir jvm #创建/usr/lib/jvm目录用来存放JDK文件</span><br><span class="line">3.cd ~ #进入hadoop用户的主目录</span><br><span class="line">4.cd /home/hadoop  #注意区分大小写字母，刚才已经通过FTP软件把JDK安装包jdk-8u162-linux-x64.tar.gz上传到该目录下</span><br><span class="line">5. sudo tar -zxvf /home/hadoop/jdk-8u162-linux-x64.tar.gz -C /usr/lib/  #把JDK文件解压到/usr/lib/jvm目录下</span><br></pre></td></tr></table></figure>
<p><img src="image-20230518175124501.png" alt="image-20230518175124501"><img src="image-20230518175734613.png" alt="image-20230518175734613"></p>
<p>JDK文件解压缩以后，可以执行如下命令到/usr/lib/jvm目录查看一下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/lib/jvm</span><br><span class="line">2.ls</span><br></pre></td></tr></table></figure>
<p>可以看到，在/usr/lib/jvm目录下有个jdk1.8.0_162目录。</p>
<p><img src="image-20230518175829044.png" alt="image-20230518175829044">下面继续执行如下命令，设置环境变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd ~</span><br><span class="line">2.vim ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>上面命令使用vim编辑器（<a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/1607-2/">查看vim编辑器使用方法</a>）打开了hadoop这个用户的环境变量配置文件，请在这个文件的开头位置，添加如下几行内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib</span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure>
<p>保存.bashrc文件并退出vim编辑器。然后，继续执行如下命令让.bashrc文件的配置立即生效：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
<p><img src="image-20230518175925063.png" alt="image-20230518175925063"><img src="image-20230518175956477.png" alt="image-20230518175956477"></p>
<p>这时，可以使用如下命令查看是否安装成功：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure>
<p>如果能够在屏幕上返回如下信息，则说明安装成功：</p>
<p><img src="image-20230518180021388.png" alt="image-20230518180021388"></p>
</li>
<li><h4 id="安装Hadoop2-7-1"><a href="#安装Hadoop2-7-1" class="headerlink" title="安装Hadoop2.7.1"></a>安装Hadoop2.7.1</h4><p>我们选择将 Hadoop 安装至 /usr/local/ 中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.sudo tar -zxvf /home/hadoop/hadoop-2.7.1.tar.gz -C /usr/local    # 解压到/usr/local中</span><br><span class="line">2.cd /usr/local/</span><br><span class="line">3. sudo mv ./hadoop-2.7.1/ ./hadoop     # 将文件夹名改为hadoop</span><br><span class="line">4.sudo chown -R hadoop ./hadoop       # 修改文件权限</span><br></pre></td></tr></table></figure>
<p><img src="image-20230518180308059.png" alt="image-20230518180308059"></p>
<p><img src="image-20230518180437612.png" alt="image-20230518180437612"></p>
<p>Hadoop 解压后即可使用。输入如下命令来检查 Hadoop 是否可用，成功则会显示 Hadoop 版本信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop</span><br><span class="line">./bin/hadoop version</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="image-20230518180542980.png" alt="image-20230518180542980"></p>
<ol>
<li><h4 id="Hadoop单机配置-非分布式"><a href="#Hadoop单机配置-非分布式" class="headerlink" title="Hadoop单机配置(非分布式)"></a>Hadoop单机配置(非分布式)</h4><p>我们将 input 文件夹中的所有文件作为输入，筛选当中符合正则表达式 dfs[a-z.]+ 的单词并统计出现的次数，最后输出结果到 output 文件夹中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hadoop</span><br><span class="line">2.mkdir ./input</span><br><span class="line">3.cp ./etc/hadoop/*.xml ./input   # 将配置文件作为输入文件</span><br><span class="line">4../bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar grep ./input ./output &#x27;dfs[a-z.]+&#x27;</span><br><span class="line">5.cat ./output/*          # 查看运行结果</span><br></pre></td></tr></table></figure>
<p>执行成功后如下所示，输出了作业的相关信息，输出的结果是符合正则的单词 dfsadmin 出现了1次</p>
<p><img src="image-20230518180754600.png" alt="image-20230518180754600"></p>
<p><img src="image-20230518180732737.png" alt="image-20230518180732737"></p>
<p>Hadoop 默认不会覆盖结果文件，因此再次运行上面实例会提示出错，需要先将 ./output 删除。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -r ./output</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="image-20230518180836594.png" alt="image-20230518180836594"></p>
<ol>
<li><h4 id="Hadoop伪分布式配置"><a href="#Hadoop伪分布式配置" class="headerlink" title="Hadoop伪分布式配置"></a>Hadoop伪分布式配置</h4><p>Hadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件。</p>
<p>Hadoop 的配置文件位于 /usr/local/hadoop/etc/hadoop/ 中，伪分布式需要修改2个配置文件core-site.xml 和 hdfs-site.xml 。Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。</p>
<p>修改配置文件core-site.xml</p>
<p><img src="image-20230518181156914.png" alt="image-20230518181156914"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/etc/hadoop</span><br><span class="line">vim core-site.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230518181050022.png" alt="image-20230518181050022"></p>
<p>同样的，修改配置文件hdfs-site.xml：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hdfs-site.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230518181136053.png" alt="image-20230518181136053"></p>
<p>Hadoop 的运行方式是由配置文件决定的（运行 Hadoop 时会读取配置文件），因此如果需要从伪分布式模式切换回非分布式模式，需要删除 core-site.xml 中的配置项。</p>
<p>此外，伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（官方教程如此），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 /tmp/hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 dfs.namenode.name.dir 和 dfs.datanode.data.dir，否则在接下来的步骤中可能会出错。</p>
<p>配置完成后，执行 NameNode 的格式化:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop</span><br><span class="line">./bin/hdfs namenode -format</span><br></pre></td></tr></table></figure>
<p>成功的话，会看到 “successfully formatted” 的提示</p>
<p><img src="image-20230518181259011.png" alt="image-20230518181259011"></p>
<p>接着开启 NameNode 和 DataNode 守护进程。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hadoop</span><br><span class="line">2../sbin/start-dfs.sh  #start-dfs.sh是个完整的可执行文件，中间没有空格</span><br></pre></td></tr></table></figure>
<p>若出现如下SSH提示，输入yes即可。</p>
<p>启动时可能会出现如下 WARN 提示：WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable WARN 提示可以忽略，并不会影响正常使用。</p>
<p><img src="image-20230518181357548.png" alt="image-20230518181357548"></p>
</li>
<li><h4 id="运行Hadoop伪分布式实例"><a href="#运行Hadoop伪分布式实例" class="headerlink" title="运行Hadoop伪分布式实例"></a>运行Hadoop伪分布式实例</h4><p>上面的单机模式，vim 例子读取的是本地数据，伪分布式读取的则是 HDFS 上的数据。要使用 HDFS，首先需要在 HDFS 中创建用户目录：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -mkdir -p /user/hadoop</span><br></pre></td></tr></table></figure>
<p>接着将 ./etc/hadoop 中的 xml 文件作为输入文件复制到分布式文件系统中，即将 /usr/local/hadoop/etc/hadoop 复制到分布式文件系统中的 /user/hadoop/input 中。我们使用的是 hadoop 用户，并且已创建相应的用户目录 /user/hadoop ，因此在命令中就可以使用相对路径如 input，其对应的绝对路径就是 /user/hadoop/input:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -mkdir input</span><br><span class="line">./bin/hdfs dfs -put ./etc/hadoop/*.xml input</span><br></pre></td></tr></table></figure>
<p>复制完成后，可以通过如下命令查看文件列表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -ls input</span><br></pre></td></tr></table></figure>
<p><img src="image-20230518182205593.png" alt="image-20230518182205593"></p>
<p>伪分布式运行 MapReduce 作业的方式跟单机模式相同，区别在于伪分布式读取的是HDFS中的文件（可以将单机步骤中创建的本地 input 文件夹，输出结果 output 文件夹都删掉来验证这一点）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar grep input output &#x27;dfs[a-z.]+&#x27;</span><br></pre></td></tr></table></figure>
<p>查看运行结果的命令（查看的是位于 HDFS 中的输出结果）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -cat output/*</span><br></pre></td></tr></table></figure>
<p>结果如下，注意到刚才我们已经更改了配置文件，所以运行结果不同。</p>
<p><img src="image-20230518182330282.png" alt="image-20230518182330282"></p>
<p>我们也可以将运行结果取回到本地：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.rm -r ./output    # 先删除本地的 output 文件夹（如果存在）</span><br><span class="line">2../bin/hdfs dfs -get output ./output     # 将 HDFS 上的 output 文件夹拷贝到本机</span><br><span class="line">3.cat ./output/*</span><br></pre></td></tr></table></figure>
<p>Hadoop 运行程序时，输出目录不能存在，否则会提示错误 “org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:9000/user/hadoop/output already exists” ，因此若要再次执行，需要执行如下命令删除 output 文件夹:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -rm -r output    # 删除 output 文件夹</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>   <img src="image-20230518182424148.png" alt="image-20230518182424148">若要关闭 Hadoop，则运行</p>
   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure>
<p>   <img src="image-20230518182509404.png" alt="image-20230518182509404"></p>
<h3 id="（三）安装MYSQL"><a href="#（三）安装MYSQL" class="headerlink" title="（三）安装MYSQL"></a>（三）安装MYSQL</h3><ol>
<li><h4 id="使用以下命令即可进行mysql安装，注意安装前先更新一下软件源以获得最新版本："><a href="#使用以下命令即可进行mysql安装，注意安装前先更新一下软件源以获得最新版本：" class="headerlink" title="使用以下命令即可进行mysql安装，注意安装前先更新一下软件源以获得最新版本："></a>使用以下命令即可进行mysql安装，注意安装前先更新一下软件源以获得最新版本：</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update  #更新软件源</span><br><span class="line">sudo apt-get install mysql-server  #安装mysql</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519111540794.png" alt="image-20230519111540794"></p>
<p>上述命令会安装以下包：<br>apparmor<br>mysql-client-5.7<br>mysql-common<br>mysql-server<br>mysql-server-5.7<br>mysql-server-core-5.7<br>因此无需再安装mysql-client等。安装过程会提示设置mysql root用户的密码，设置完成后等待自动安装即可。默认安装完成就启动了mysql</p>
<p>启动和关闭mysql服务器：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service mysql start</span><br><span class="line">service mysql stop</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519111625591.png" alt="image-20230519111625591"></p>
<p>确认是否启动成功，mysql节点处于LISTEN状态表示启动成功：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo netstat -tap | grep mysql</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519111712578.png" alt="image-20230519111712578"></p>
<p>进入mysql shell界面：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mysql -u root -p</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519111759996.png" alt="image-20230519111759996"></p>
<p>解决利用sqoop导入MySQL中文乱码的问题（可以插入中文，但不能用sqoop导入中文）<br>导致导入时中文乱码的原因是character_set_server默认设置是latin1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show variables like &quot;char%&quot;;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519111936790.png" alt="image-20230519111936790"></p>
<p>(1)编辑配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf</span><br></pre></td></tr></table></figure>
<p>(2)在[mysqld]下添加一行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">character_set_server=utf8</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519112148418.png" alt="image-20230519112148418"></p>
<p>(3)重启MySQL服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql restart</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519112305745.png" alt="image-20230519112305745"></p>
<p>(4)登陆MySQL，并查看MySQL目前设置的编码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mysql -u root -p</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show variables like &quot;char%&quot;;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519112356059.png" alt="image-20230519112356059"></p>
</li>
<li><h4 id="下载mysql-jdbc包"><a href="#下载mysql-jdbc包" class="headerlink" title="下载mysql jdbc包"></a>下载mysql jdbc包</h4><p><img src="image-20230519112632655.png" alt="image-20230519112632655"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. sudo tar -zxvf /home/hadoop/mysql-connector-java.gz  #解压</span><br><span class="line">2.cp mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar  /usr/local/hive/lib #将mysql-connector-java-5.1.40-bin.jar拷贝到/usr/local/hive/lib目录下</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519112916846.png" alt="image-20230519112916846"></p>
<p><img src="image-20230519113044007.png" alt="image-20230519113044007"></p>
<p><img src="image-20230519113115642.png" alt="image-20230519113115642"></p>
</li>
<li><h4 id="启动并登陆mysql-shell"><a href="#启动并登陆mysql-shell" class="headerlink" title="启动并登陆mysql shell"></a>启动并登陆mysql shell</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service mysql start #启动mysql服务</span><br><span class="line">sudo mysql -u root -p  #登陆shell界面</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519113303077.png" alt="image-20230519113303077"></p>
</li>
<li><h4 id="新建hive数据库"><a href="#新建hive数据库" class="headerlink" title="新建hive数据库"></a>新建hive数据库</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create database hive;    #这个hive数据库与hive-site.xml中localhost:3306/hive的hive对应，用来保存hive元数据</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519113451943.png" alt="image-20230519113451943"></p>
</li>
<li><h4 id="配置mysql允许hive接入"><a href="#配置mysql允许hive接入" class="headerlink" title="配置mysql允许hive接入"></a>配置mysql允许hive接入</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; grant all on *.* to hive@localhost identified by &#x27;hive&#x27;;   #将所有数据库的所有表的所有权限赋给hive用户，后面的hive是配置hive-site.xml中配置的连接密码</span><br><span class="line">mysql&gt; flush privileges;  #刷新mysql系统权限关系表</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519113555294.png" alt="image-20230519113555294"></p>
</li>
<li><h4 id="启动hive"><a href="#启动hive" class="headerlink" title="启动hive"></a>启动hive</h4><p>启动hive之前，请先启动hadoop集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">hive   #启动hive</span><br></pre></td></tr></table></figure>
<p>注意，我们这里已经配置了PATH，所以，不要把start-all.sh和hive命令的路径加上。如果没有配置PATH，请加上路径才能运行命令，比如，本教程Hadoop安装目录是“/usr/local/hadoop”，Hive的安装目录是“/usr/local/hive”，因此，启动hadoop和hive，也可以使用下面带路径的方式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop #进入Hadoop安装目录</span><br><span class="line">./sbin/start-dfs.sh</span><br><span class="line">cd /usr/local/hive</span><br><span class="line">./bin/hive</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519113853472.png" alt="image-20230519113853472"></p>
<p>可以在里面输入SQL语句，如果要退出Hive交互式执行环境，可以输入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exit;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="（四）安装HBase"><a href="#（四）安装HBase" class="headerlink" title="（四）安装HBase"></a>（四）安装HBase</h3><ol>
<li><h4 id="hbase安装"><a href="#hbase安装" class="headerlink" title="hbase安装"></a>hbase安装</h4><p>1.1 解压安装包hbase-2.2.2-bin.tar.gz至路径 /usr/local，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxvf /home/hadoop/hbase-1.1.2-bin.tar.gz -C /usr/local</span><br></pre></td></tr></table></figure>
<p>1.2 将解压的文件名hbase-2.2.2改为hbase，以方便使用，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mv /usr/local/hbase-1.1.2 /usr/local/hbase</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519102717668.png" alt="image-20230519102717668"></p>
<p>1.3 配置环境变量</p>
<p>将hbase下的bin目录添加到path中，这样，启动hbase就无需到/usr/local/hbase目录下，大大的方便了hbase的使用。教程下面的部分还是切换到了/usr/local/hbase目录操作，有助于初学者理解运行过程，熟练之后可以不必切换。</p>
<p>编辑~/.bashrc文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>如果没有引入过PATH请在~/.bashrc文件尾行添加如下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/usr/local/hbase/bin</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519102827892.png" alt="image-20230519102827892"></p>
<p>编辑完成后，再执行source命令使上述配置在当前终端立即生效，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519102901810.png" alt="image-20230519102901810"></p>
<p>1.4 添加HBase权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local</span><br><span class="line">sudo chown -R hadoop ./hbase    #将hbase下的所有文件的所有者改为hadoop</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519103002599.png" alt="image-20230519103002599"></p>
<p>1.5 查看HBase版本，确定hbase安装成功,命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/hbase/bin/hbase version</span><br></pre></td></tr></table></figure>
<p>命令执行后，输出信息截图如下：</p>
<p><img src="image-20230519103131786.png" alt="image-20230519103131786"></p>
</li>
<li><h4 id="hbase配置"><a href="#hbase配置" class="headerlink" title="hbase配置"></a>hbase配置</h4><p>2.1 配置/usr/local/hbase/conf/hbase-env.sh 命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /usr/local/hbase/conf/hbase-env.sh</span><br></pre></td></tr></table></figure>
<p>配置JAVA_HOME，HBASE_CLASSPATH，HBASE_MANAGES_ZK.</p>
<p>HBASE_CLASSPATH设置为本机Hadoop安装目录下的conf目录（即/usr/local/hadoop/conf）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_162</span><br><span class="line">export HBASE_CLASSPATH=/usr/local/hadoop/conf </span><br><span class="line">export HBASE_MANAGES_ZK=true</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519103522037.png" alt="image-20230519103522037"></p>
<p><img src="image-20230519103539892.png" alt="image-20230519103539892"></p>
<p>2.2 配置/usr/local/hbase/conf/hbase-site.xml</p>
<p>用命令vi打开并编辑hbase-site.xml，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /usr/local/hbase/conf/hbase-site.xml</span><br></pre></td></tr></table></figure>
<p>修改hbase.rootdir，指定HBase数据在HDFS上的存储路径；将属性hbase.cluter.distributed设置为true。假设当前Hadoop集群运行在伪分布式模式下，在本机上运行，且NameNode运行在9000端口。</p>
</li>
</ol>
   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">​    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">​        &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">​        &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">​    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">​    &lt;property&gt;</span><br><span class="line"></span><br><span class="line">​        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">​        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">​    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p>   hbase.rootdir指定HBase的存储目录；hbase.cluster.distributed设置集群处于分布式模式.</p>
<p>   截图如下：</p>
<p>   <img src="image-20230519103840982.png" alt="image-20230519103840982"></p>
<ol>
<li><h4 id="测试运行HBase"><a href="#测试运行HBase" class="headerlink" title="测试运行HBase"></a>测试运行HBase</h4><p>第一步：首先登陆ssh，之前设置了无密码登陆，因此这里不需要密码；再切换目录至/usr/local/hadoop ；再启动hadoop，如果已经启动hadoop请跳过此步骤。命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost</span><br><span class="line">cd /usr/local/hadoop</span><br><span class="line">./sbin/start-dfs.sh</span><br><span class="line">jps   #能看到NameNode,DataNode和SecondaryNameNode都已经成功启动，表示hadoop启动成功</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519104158678.png" alt="image-20230519104158678"></p>
<p>第二步：切换目录至/usr/local/hbase;再启动HBase.命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hbase</span><br><span class="line">bin/start-hbase.sh</span><br><span class="line">jps  #启动成功，输入命令jps，看到以下界面说明hbase启动成功</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519104353783.png" alt="image-20230519104353783"></p>
<p>进入shell界面：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hbase shell</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519104445276.png" alt="image-20230519104445276"></p>
<p>停止HBase运行,命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/stop-hbase.sh</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519104633970.png" alt="image-20230519104633970"></p>
</li>
</ol>
<h3 id="（五）安装Hive"><a href="#（五）安装Hive" class="headerlink" title="（五）安装Hive"></a>（五）安装Hive</h3><ol>
<li><h4 id="下载并加压hive安装包"><a href="#下载并加压hive安装包" class="headerlink" title="下载并加压hive安装包"></a>下载并加压hive安装包</h4><p><img src="image-20230519110040430.png" alt="image-20230519110040430"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.sudo tar -zxvf /home/hadoop/apache-hive-1.2.1-bin.tar.gz -C /usr/local # 解压到/usr/local中</span><br><span class="line">2.cd /usr/local</span><br><span class="line">3.sudo mv apache-hive-1.2.1-bin hive       # 将文件夹名改为hive</span><br><span class="line">4.sudo chown -R hadoop:hadoop hive            # 修改文件权限</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519110443084.png" alt="image-20230519110443084"></p>
</li>
<li><h4 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h4><p>为了方便使用，我们把hive命令加入到环境变量中去，<br>请使用vim编辑器打开.bashrc文件，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>在该文件最前面一行添加如下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519110818121.png" alt="image-20230519110818121"></p>
<p>HADOOP_HOME需要被配置成你机器上Hadoop的安装路径，比如这里是安装在/usr/local./hadoop目录。<br>保存退出后，运行如下命令使配置立即生效：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519110847306.png" alt="image-20230519110847306"></p>
</li>
<li><h4 id="修改-usr-local-hive-conf下的hive-site-xml"><a href="#修改-usr-local-hive-conf下的hive-site-xml" class="headerlink" title="修改/usr/local/hive/conf下的hive-site.xml"></a>修改/usr/local/hive/conf下的hive-site.xml</h4><p>执行如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hive/conf</span><br><span class="line">mv hive-default.xml.template hive-default.xml</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519111203011.png" alt="image-20230519111203011"></p>
<p>上面命令是将hive-default.xml.template重命名为hive-default.xml；<br>然后，使用vim编辑器新建一个配置文件hive-site.xml，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hive/conf</span><br><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure>
<p>在hive-site.xml中添加如下配置信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519111237247.png" alt="image-20230519111237247"></p>
</li>
<li><h4 id="在安装完mysql并配置完成之后启动hive"><a href="#在安装完mysql并配置完成之后启动hive" class="headerlink" title="在安装完mysql并配置完成之后启动hive"></a>在安装完mysql并配置完成之后启动hive</h4><p>启动hive之前，请先启动hadoop集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">hive   #启动hive</span><br></pre></td></tr></table></figure>
<p>注意，我们这里已经配置了PATH，所以，不要把start-all.sh和hive命令的路径加上。如果没有配置PATH，请加上路径才能运行命令，比如，本教程Hadoop安装目录是“/usr/local/hadoop”，Hive的安装目录是“/usr/local/hive”，因此，启动hadoop和hive，也可以使用下面带路径的方式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop #进入Hadoop安装目录</span><br><span class="line">./sbin/start-dfs.sh</span><br><span class="line">cd /usr/local/hive</span><br><span class="line">./bin/hive</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519113853472.png" alt="image-20230519113853472"></p>
<p>可以在里面输入SQL语句，如果要退出Hive交互式执行环境，可以输入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exit;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="（六）安装Eclipse"><a href="#（六）安装Eclipse" class="headerlink" title="（六）安装Eclipse"></a>（六）安装Eclipse</h3><p><img src="image-20230519101114286.png" alt="image-20230519101114286"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxvf /home/hadoop/eclipse-4.7.0-linux..gz -C /usr/local    #解压到/usr/local</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519101303827.png" alt="image-20230519101303827"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/eclipse</span><br><span class="line">./eclipse        #运行eclipse</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519101440184.png" alt="image-20230519101440184"></p>
<h3 id="（七）安装sqoop"><a href="#（七）安装sqoop" class="headerlink" title="（七）安装sqoop"></a>（七）安装sqoop</h3><ol>
<li><h4 id="下载并解压sqoop1-4-6"><a href="#下载并解压sqoop1-4-6" class="headerlink" title="下载并解压sqoop1.4.6"></a>下载并解压sqoop1.4.6</h4><p>下面执行以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ~  #进入当前用户的用户目录</span><br><span class="line">sudo tar -zxvf /home/hadoop/sqoop-1.4.6.bin__had.gz -C /usr/local              #解压安装文件</span><br><span class="line">cd /usr/local</span><br><span class="line">sudo mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop #修改文件名</span><br><span class="line">sudo chown -R hadoop:hadoop sqoop #修改文件夹属主，如果你当前登录用户名不是hadoop，请修改成你自己的用户名</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519204202670.png" alt="image-20230519204202670"><img src="image-20230519204459467.png" alt="image-20230519204459467"></p>
</li>
<li><h4 id="修改配置文件sqoop-env-sh"><a href="#修改配置文件sqoop-env-sh" class="headerlink" title="修改配置文件sqoop-env.sh"></a>修改配置文件sqoop-env.sh</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd sqoop/conf/</span><br><span class="line">cat sqoop-env-template.sh  &gt;&gt; sqoop-env.sh  #将sqoop-env-template.sh复制一份并命名为sqoop-env.sh</span><br><span class="line">vim sqoop-env.sh #编辑sqoop-env.sh</span><br></pre></td></tr></table></figure>
<p>修改sqoop-env.sh的如下信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_COMMON_HOME=/usr/local/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME=/usr/local/hadoop</span><br><span class="line">export HBASE_HOME=/usr/local/hbase</span><br><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line"></span><br><span class="line">#export ZOOCFGDIR= #如果读者配置了ZooKeeper,也需要在此配置ZooKeeper的路径</span><br></pre></td></tr></table></figure>
<p><img src="image-20230521150512380.png" alt="image-20230519204821964"></p>
</li>
<li><h4 id="配置环境变量-1"><a href="#配置环境变量-1" class="headerlink" title="配置环境变量"></a>配置环境变量</h4><p>打开当前用户的环境变量配置文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>在配置文件第一行键入如下信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export SQOOP_HOME=/usr/local/sqoop</span><br><span class="line">export PATH=$PATH:$SBT_HOME/bin:$SQOOP_HOME/bin</span><br><span class="line">export CLASSPATH=$CLASSPATH:$SQOOP_HOME/lib</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519204906298.png" alt="image-20230519204906298"></p>
<p>保存该文件，退出vim编辑器。<br>然后，执行下面命令让配置文件立即生效:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
<p><img src="image-20230519204935905.png" alt="image-20230519204935905"></p>
</li>
<li><h4 id="将mysql驱动包拷贝到-SQOOP-HOME-lib"><a href="#将mysql驱动包拷贝到-SQOOP-HOME-lib" class="headerlink" title="将mysql驱动包拷贝到$SQOOP_HOME/lib"></a>将mysql驱动包拷贝到$SQOOP_HOME/lib</h4><p>下面要把MySQL驱动程序拷贝到$SQOOP_HOME/lib目录下，首先请在Linux系统的浏览器中请点击<a target="_blank" rel="noopener" href="http://www.mysql.com/downloads/connector/j/">mysql驱动包下载地址</a>下载驱动包。下载后，一般文件会被浏览器默认放置在当前用户的下载目录下，本教程采用hadoop用户登录Linux系统，因此，下载文件被默认放置在“/home/hadoop/下载”目录下面。<br>下面执行命令拷贝文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop #切换到下载路径，如果你下载的文件不在这个目录下，请切换到下载文件所保存的目录</span><br><span class="line">sudo tar -zxvf /home/hadoop/mysql-connector-java-5.1.49.tar.gz  #解压mysql驱动包</span><br><span class="line">ls #这时就可以看到解压缩后得到的目录mysql-connector-java-5.1.49</span><br><span class="line"></span><br><span class="line">cp /home/hadoop/mysql-connector-java-5.1.49/mysql-connector-java-5.1.49.jar /usr/local/sqoop/lib</span><br><span class="line"></span><br><span class="line"> cp /home/hadoop/mysql-connector-java-5.1.49/mysql-connector-java-5.1.49-bin.jar /usr/local/sqoop/lib</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="image-20230521152252175.png" alt="image-20230521152252175"></p>
</li>
<li><h4 id="测试与MySQL的连接"><a href="#测试与MySQL的连接" class="headerlink" title="测试与MySQL的连接"></a>测试与MySQL的连接</h4><p>首先请确保mysql服务已经启动了，如果没有启动，请执行下面命令启动：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql start</span><br></pre></td></tr></table></figure>
<p>然后就可以测试sqoop与MySQL之间的连接是否成功：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases --connect jdbc:mysql://127.0.0.1:3306/ --username root -P</span><br></pre></td></tr></table></figure>
<p>mysql的数据库列表显示在屏幕上表示连接成功，如下图：</p>
<p><img src="image-20230521155219093.png" alt="image-20230521155219093"></p>
<p>错误分析：</p>
<p>一开始报错</p>
<p><img src="image-20230521155422875.png" alt="image-20230521155422875"></p>
<p>此时我用的是mysql-connector-java-5.1.40.tar.gz ，但这个里面没有mysql-connector-java-5.1.40.jar 包，后来换了mysql-connector-java-5.1.49.tar.gz这个，但仍然报错，我进入MySQL查看了一下MySQL中的各个用户，发现root用户没有密码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select user,host,authentication_string,password_expired from user;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230521155730450.png" alt="image-20230521155730450"></p>
<p>然后我将root用户删除重新创建了root用户</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">drop user root@localhost;</span><br><span class="line">flush privileges;</span><br><span class="line">create user root@localhost identified by &#x27;root&#x27;;</span><br><span class="line">grant all privileges on *.* to root@localhost identified by &#x27;root&#x27;;</span><br><span class="line"> flush privileges;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230521155858116.png" alt="image-20230521155858116"></p>
<p>再次连接成功了</p>
<p><img src="image-20230521155926497.png" alt="image-20230521155926497"></p>
</li>
</ol>
<h2 id="二-网站用户行为分析"><a href="#二-网站用户行为分析" class="headerlink" title="二.网站用户行为分析"></a>二.网站用户行为分析</h2><h3 id="一-本地数据集上传到数据仓库Hive"><a href="#一-本地数据集上传到数据仓库Hive" class="headerlink" title="一. 本地数据集上传到数据仓库Hive"></a>一. 本地数据集上传到数据仓库Hive</h3><h4 id="1-实验数据集的下载"><a href="#1-实验数据集的下载" class="headerlink" title="1. 实验数据集的下载"></a>1. 实验数据集的下载</h4><p>请登录Linux系统（本教程统一采用hadoop用户登录），用xftp将数据下载到相应目录下</p>
<p><img src="image-20230520205801969.png" alt="image-20230520205801969"></p>
<p>现在，请在Linux系统中打开一个终端（可以使用快捷键Ctrl+Alt+T），执行下面命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop</span><br><span class="line">ls</span><br></pre></td></tr></table></figure>
<p><img src="image-20230520205838037.png" alt="image-20230520205838037"></p>
<p>通过上面命令，就进入到了user.zip文件所在的目录，并且可以看到有个user.zip文件。</p>
<p>下面需要把user.zip进行解压缩，我们需要首先建立一个用于运行本案例的目录bigdatacase，请执行以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local</span><br><span class="line">2.ls</span><br><span class="line">3.sudo mkdir bigdatacase</span><br><span class="line">4.//这里会提示你输入当前用户（本教程是hadoop用户名）的密码</span><br><span class="line">5.//下面给hadoop用户赋予针对bigdatacase目录的各种操作权限</span><br><span class="line">6.sudo chown -R hadoop:hadoop ./bigdatacase</span><br><span class="line">7.cd bigdatacase</span><br><span class="line">8.//下面创建一个dataset目录，用于保存数据集</span><br><span class="line">9. sudo mkdir dataset</span><br><span class="line">10.//下面就可以解压缩user.zip文件</span><br><span class="line">11.cd ~  //表示进入hadoop用户的目录</span><br><span class="line">12.ls</span><br><span class="line">13.unzip user.zip -d /usr/local/bigdatacase/dataset</span><br><span class="line">14.cd /usr/local/bigdatacase/dataset</span><br><span class="line">15.ls</span><br></pre></td></tr></table></figure>
<p><img src="image-20230520210442259.png" alt="image-20230520210442259"></p>
<p>现在你就可以看到在dataset目录下有两个文件：raw_user.csv和small_user.csv。<br>我们执行下面命令取出前面5条记录看一下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">head -5 raw_user.csv</span><br></pre></td></tr></table></figure>
<p>可以看到，前5行记录如下：</p>
<p><img src="image-20230520210516775.png" alt="image-20230520210516775"></p>
<p>可以看出，每行记录都包含5个字段，数据集中的字段及其含义如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">user_id（用户id）</span><br><span class="line">item_id(商品id)</span><br><span class="line">behaviour_type（包括浏览、收藏、加购物车、购买，对应取值分别是1、2、3、4）</span><br><span class="line">user_geohash(用户地理位置哈希值，有些记录中没有这个字段值，所以后面我们会用脚本做数据预处理时把这个字段全部删除)</span><br><span class="line">item_category（商品分类）</span><br><span class="line">time（该记录产生时间）</span><br></pre></td></tr></table></figure>
<h4 id="2-数据集的预处理"><a href="#2-数据集的预处理" class="headerlink" title="2.  数据集的预处理"></a>2.  数据集的预处理</h4><ol>
<li><p>删除文件第一行记录，即字段名称<br>raw_user和small_user中的第一行都是字段名称，我们在文件中的数据导入到数据仓库Hive中时，不需要第一行字段名称，因此，这里在做数据预处理时，删除第一行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/bigdatacase/dataset</span><br><span class="line">2.//下面删除raw_user中的第1行</span><br><span class="line">3.sed -i &#x27;1d&#x27; raw_user.csv //1d表示删除第1行，同理，3d表示删除第3行，nd表示删除第n行</span><br><span class="line">4.//下面删除small_user中的第1行</span><br><span class="line">5.sed -i &#x27;1d&#x27; small_user.csv</span><br><span class="line">6.//下面再用head命令去查看文件的前5行记录，就看不到字段名称这一行了</span><br><span class="line">7.head -5 raw_user.csv</span><br><span class="line">8.head -5 small_user.csv</span><br></pre></td></tr></table></figure>
<p><img src="image-20230520211434884.png" alt="image-20230520211434884"></p>
<p>接下来的操作中，我们都是用small_user.csv这个小数据集进行操作，这样可以节省时间。等所有流程都跑通以后，你就可以使用大数据集raw_user.csv去测试一遍了。</p>
</li>
<li><p>对字段进行预处理</p>
<p>下面对数据集进行一些预处理，包括为每行记录增加一个id字段（让记录具有唯一性）、增加一个省份字段（用来后续进行可视化分析），并且丢弃user_geohash字段（后面分析不需要这个字段）。<br>下面我们要建一个脚本文件pre_deal.sh，请把这个脚本文件放在dataset目录下，和数据集small_user.csv放在同一个目录下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/bigdatacase/dataset</span><br><span class="line">2.vim pre_deal.sh</span><br></pre></td></tr></table></figure>
<p>上面使用vim编辑器新建了一个pre_deal.sh脚本文件，请在这个脚本文件中加入下面代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">#下面设置输入文件，把用户执行pre_deal.sh命令时提供的第一个参数作为输入文件名称</span><br><span class="line">infile=$1</span><br><span class="line">#下面设置输出文件，把用户执行pre_deal.sh命令时提供的第二个参数作为输出文件名称</span><br><span class="line">outfile=$2</span><br><span class="line">#注意！！最后的$infile &gt; $outfile必须跟在&#125;’这两个字符的后面</span><br><span class="line">awk -F &quot;,&quot; &#x27;BEGIN&#123;</span><br><span class="line">        srand();</span><br><span class="line">        id=0;</span><br><span class="line">        Province[0]=&quot;山东&quot;;Province[1]=&quot;山西&quot;;Province[2]=&quot;河南&quot;;Province[3]=&quot;河北&quot;;Province[4]=&quot;陕西&quot;;Province[5]=&quot;内蒙古&quot;;Province[6]=&quot;上海市&quot;;</span><br><span class="line">        Province[7]=&quot;北京市&quot;;Province[8]=&quot;重庆市&quot;;Province[9]=&quot;天津市&quot;;Province[10]=&quot;福建&quot;;Province[11]=&quot;广东&quot;;Province[12]=&quot;广西&quot;;Province[13]=&quot;云南&quot;; </span><br><span class="line">        Province[14]=&quot;浙江&quot;;Province[15]=&quot;贵州&quot;;Province[16]=&quot;新疆&quot;;Province[17]=&quot;西藏&quot;;Province[18]=&quot;江西&quot;;Province[19]=&quot;湖南&quot;;Province[20]=&quot;湖北&quot;;</span><br><span class="line">        Province[21]=&quot;黑龙江&quot;;Province[22]=&quot;吉林&quot;;Province[23]=&quot;辽宁&quot;; Province[24]=&quot;江苏&quot;;Province[25]=&quot;甘肃&quot;;Province[26]=&quot;青海&quot;;Province[27]=&quot;四川&quot;;</span><br><span class="line">        Province[28]=&quot;安徽&quot;; Province[29]=&quot;宁夏&quot;;Province[30]=&quot;海南&quot;;Province[31]=&quot;香港&quot;;Province[32]=&quot;澳门&quot;;Province[33]=&quot;台湾&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    &#123;</span><br><span class="line">        id=id+1;</span><br><span class="line">        value=int(rand()*34);       </span><br><span class="line">        print id&quot;\t&quot;$1&quot;\t&quot;$2&quot;\t&quot;$3&quot;\t&quot;$5&quot;\t&quot;substr($6,1,10)&quot;\t&quot;Province[value]</span><br><span class="line">    &#125;&#x27; $infile &gt; $outfile</span><br></pre></td></tr></table></figure>
<p><img src="image-20230520211514736.png" alt="image-20230520211514736"></p>
<p>上面的代码的基本形式是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk -F &quot;,&quot; &#x27;处理逻辑&#x27; $infile &gt; $outfile</span><br></pre></td></tr></table></figure>
<p>使用awk可以逐行读取输入文件，并对逐行进行相应操作。其中，-F参数用于指出每行记录的不同字段之间用什么字符进行分割，这里是用逗号进行分割。处理逻辑代码需要用两个英文单引号引起来。 $infile是输入文件的名称，我们这里会输入raw_user.csv，$outfile表示处理结束后输出的文件名称，我们后面会使用user_table.txt作为输出文件名称。</p>
<p>在上面的pre_deal.sh代码的处理逻辑部分，srand()用于生成随机数的种子，id是我们为数据集新增的一个字段，它是一个自增类型，每条记录增加1，这样可以保证每条记录具有唯一性。我们会为数据集新增一个省份字段，用来进行后面的数据可视化分析，为了给每条记录增加一个省份字段的值，这里，我们首先用Province[]数组用来保存全国各个省份信息，然后，在遍历数据集raw_user.csv的时候，每当遍历到其中一条记录，使用value=int(rand()*34)语句随机生成一个0-33的整数，作为Province省份值，然后从Province[]数组当中获取省份名称，增加到该条记录中。</p>
<p>substr($6,1,10)这个语句是为了截取时间字段time的年月日，方便后续存储为date格式。awk每次遍历到一条记录时，每条记录包含了6个字段，其中，第6个字段是时间字段，substr($6,1,10)语句就表示获取第6个字段的值，截取前10个字符，第6个字段是类似”2014-12-08 18″这样的字符串（也就是表示2014年12月8日18时），substr($6,1,10)截取后，就丢弃了小时，只保留了年月日。<br>另外，在print id”\t”$1″\t”$2″\t”$3″\t”$5″\t”substr($6,1,10)”\t”Province[value]这行语句中，我们丢弃了每行记录的第4个字段，所以，没有出现$4。我们生成后的文件是“\t”进行分割，这样，后续我们去查看数据的时候，效果让人看上去更舒服，每个字段在排版的时候会对齐显示，如果用逗号分隔，显示效果就比较乱。</p>
<p>最后，保存pre_deal.sh代码文件，退出vim编辑器。<br>下面就可以执行pre_deal.sh脚本文件，来对small_user.csv进行数据预处理，命令如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/bigdatacase/dataset</span><br><span class="line">2.bash ./pre_deal.sh small_user.csv user_table.txt  </span><br></pre></td></tr></table></figure>
<p>可以使用head命令查看生成的user_table.txt，不要直接打开，文件过大，会出错，下面查看前10行数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.head -10 user_table.txt</span><br></pre></td></tr></table></figure>
<p>可以得到如下结果：</p>
<p><img src="image-20230520211813834.png" alt="image-20230520211813834"></p>
</li>
<li><p>导入数据库</p>
<p>下面要把user_table.txt中的数据最终导入到数据仓库Hive中。为了完成这个操作，我们会首先把user_table.txt上传到分布式文件系统HDFS中，然后，在Hive中创建一个外部表，完成数据的导入。</p>
<p>a.启动HDFS<br>HDFS是Hadoop的核心组件，因此，需要使用HDFS，必须安装Hadoop。这里假设你已经安装了Hadoop，本教程使用的是Hadoop2.7.1版本，安装目录是“/usr/local/hadoop”。</p>
<p>下面，请登录Linux系统，打开一个终端，执行下面命令启动Hadoop：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hadoop</span><br><span class="line">2../sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p>然后，执行jps命令看一下当前运行的进程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>
<p>如果出现下面这些进程，说明Hadoop启动成功了:</p>
<p><img src="image-20230520212108328.png" alt="image-20230520212108328"></p>
<p>b.把user_table.txt上传到HDFS中<br>现在，我们要把Linux本地文件系统中的user_table.txt上传到分布式文件系统HDFS中，存放在HDFS中的“/bigdatacase/dataset”目录下。<br>首先，请执行下面命令，在HDFS的根目录下面创建一个新的目录bigdatacase，并在这个目录下创建一个子目录dataset，如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hadoop</span><br><span class="line">2../bin/hdfs dfs -mkdir -p /bigdatacase/dataset</span><br></pre></td></tr></table></figure>
<p><img src="image-20230520212337614.png" alt="image-20230520212337614"></p>
<p>然后，把Linux本地文件系统中的user_table.txt上传到分布式文件系统HDFS的“/bigdatacase/dataset”目录下，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hadoop</span><br><span class="line">2../bin/hdfs dfs -put /usr/local/bigdatacase/dataset/user_table.txt /bigdatacase/dataset</span><br></pre></td></tr></table></figure>
<p>下面可以查看一下HDFS中的user_table.txt的前10条记录，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hadoop</span><br><span class="line">2../bin/hdfs dfs -cat /bigdatacase/dataset/user_table.txt | head -10</span><br></pre></td></tr></table></figure>
<p><img src="image-20230520212430962.png" alt="image-20230520212430962"></p>
<p>c.在Hive上创建数据库<br>关于什么是数据仓库Hive？Hive的运行基本原理是什么？如何开展Hive简单编程实践？<br>本案例教程需要安装数据仓库Hive，这里假设你已经完成了Hive的安装，并且使用MySQL数据库保存Hive的元数据。本教程安装的是Hive2.1.0版本，安装目录是“/usr/local/hive”。<br>下面，请在Linux系统中，再新建一个终端（可以在刚才已经建好的终端界面的左上角，点击“终端”菜单，在弹出的子菜单中选择“新建终端”）。因为需要借助于MySQL保存Hive的元数据，所以，请首先启动MySQL数据库：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.service mysql start  //可以在Linux的任何目录下执行该命令</span><br></pre></td></tr></table></figure>
<p>由于Hive是基于Hadoop的数据仓库，使用HiveQL语言撰写的查询语句，最终都会被Hive自动解析成MapReduce任务由Hadoop去具体执行，因此，需要启动Hadoop，然后再启动Hive。由于前面我们已经启动了Hadoop，所以，这里不需要再次启动Hadoop。下面，在这个新的终端中执行下面命令进入Hive：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hive</span><br><span class="line">2../bin/hive   //启动Hive</span><br></pre></td></tr></table></figure>
<p>启动成功以后，就进入了“hive&gt;”命令提示符状态，可以输入类似SQL语句的HiveQL语句。</p>
<p><img src="image-20230520212634858.png" alt="image-20230520212634858"></p>
<p><img src="image-20230520212650120.png" alt="image-20230520212650120"></p>
<p>下面，我们要在Hive中创建一个数据库dblab，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.hive&gt;  create database dblab;</span><br><span class="line">2.hive&gt;  use dblab;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230520212714795.png" alt="image-20230520212714795"></p>
<p>d.创建外部表<br>关于数据仓库Hive的内部表和外部表的区别，本教程采用外部表方式。<br>这里我们要在数据库dblab中创建一个外部表bigdata_user，它包含字段（id, uid, item_id, behavior_type, item_category, date, province），请在hive命令提示符下输入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;  CREATE EXTERNAL TABLE dblab.bigdata_user(id INT,uid STRING,item_id STRING,behavior_type INT,item_category STRING,visit_date DATE,province STRING) COMMENT &#x27;Welcome to xmu dblab!&#x27; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; STORED AS TEXTFILE LOCATION &#x27;/bigdatacase/dataset&#x27;;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230520212804298.png" alt="image-20230520212804298"> <em>**\</em>e.查询数据<br>上面已经成功把HDFS中的“/bigdatacase/dataset”目录下的数据加载到了数据仓库Hive中，我们现在可以使用下面命令查询一下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.hive&gt;  select * from bigdata_user limit 10;</span><br><span class="line">2.hive&gt;  select behavior_type from bigdata_user limit 10;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230520212915936.png" alt="image-20230520212915936"></p>
</li>
</ol>
<h3 id="二-Hive数据分析"><a href="#二-Hive数据分析" class="headerlink" title="二. Hive数据分析"></a>二. Hive数据分析</h3><h4 id="1-操作Hive"><a href="#1-操作Hive" class="headerlink" title="1.操作Hive"></a>1.操作Hive</h4><p>请登录Linux系统（本教程统一采用hadoop用户名登录系统），然后，打开一个终端（可以按快捷键Ctrl+Alt+T）。<br>本教程中，Hadoop的安装目录是“/usr/local/hadoop”，Hive的安装目录是“/usr/local/hive”。<br>因为需要借助于MySQL保存Hive的元数据，所以，请首先启动MySQL数据库，请在终端中输入下面命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql start  //可以在Linux的任何目录下执行该命令</span><br></pre></td></tr></table></figure>
<p>由于Hive是基于Hadoop的数据仓库，使用HiveQL语言撰写的查询语句，最终都会被Hive自动解析成MapReduce任务由Hadoop去具体执行，因此，需要启动Hadoop，然后再启动Hive。</p>
<p>请执行下面命令启动Hadoop（如果你已经启动了Hadoop就不用再次启动了）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hadoop</span><br><span class="line">2../sbin/start-all.sh</span><br><span class="line">3.jps          #查看一下进程</span><br></pre></td></tr></table></figure>
<p>如果出现下面这些进程，说明Hadoop启动成功了：</p>
<p><img src="image-20230521132143873.png" alt="image-20230521132143873"></p>
<p>下面，继续执行下面命令启动进入Hive：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hive</span><br><span class="line">2../bin/hive   //启动Hive</span><br></pre></td></tr></table></figure>
<p>通过上述过程，我们就完成了MySQL、Hadoop和Hive三者的启动。<br>启动成功以后，就进入了“hive&gt;”命令提示符状态，可以输入类似SQL语句的HiveQL语句。</p>
<p><img src="image-20230521132230389.png" alt="image-20230521132230389"><img src="image-20230521132258269.png" alt="image-20230521132258269"></p>
<p>然后，在“hive&gt;”命令提示符状态下执行下面命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.hive&gt; use dblab; //使用dblab数据库</span><br><span class="line">2.hive&gt; show tables; //显示数据库中所有表。</span><br><span class="line">3.hive&gt; show create table bigdata_user; //查看bigdata_user表的各种属性；</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p><img src="image-20230521132408219.png" alt="image-20230521132408219"></p>
<p>可以执行下面命令查看表的简单结构：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.hive&gt; desc bigdata_user;</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p><img src="image-20230521132431320.png" alt="image-20230521132431320"></p>
<h4 id="2-简单查询分析"><a href="#2-简单查询分析" class="headerlink" title="2.简单查询分析"></a>2.简单查询分析</h4><p>先测试一下简单的指令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.hive&gt; select behavior_type from bigdata_user limit 10;//查看前10位用户对商品的行为</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p><img src="image-20230521132454930.png" alt="image-20230521132454930"></p>
<p>如果要查出每位用户购买商品时的多种信息，输出语句格式为 select 列1，列2，….，列n from 表名；<br>比如我们现在查询前20位用户购买商品时的时间和商品的种类</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.hive&gt; select visit_date,item_category from bigdata_user limit 20;</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p><img src="image-20230521132521517.png" alt="image-20230521132521517"></p>
<p>有时我们在表中查询可以利用嵌套语句，如果列名太复杂可以设置该列的别名，以简化我们操作的难度，以下我们可以举个例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.hive&gt; select e.bh, e.it  from (select behavior_type as bh, item_category as it from bigdata_user) as e  limit 20;</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p><img src="image-20230521132551215.png" alt="image-20230521132551215"></p>
<p>这里简单的做个讲解，behavior_type as bh ,item_category as it就是把behavior_type 设置别名 bh ,item_category 设置别名 it，FROM的括号里的内容我们也设置了别名e，这样调用时用e.bh,e.it,可以简化代码。</p>
<h4 id="3-查询条数统计分析"><a href="#3-查询条数统计分析" class="headerlink" title="3.查询条数统计分析"></a>3.查询条数统计分析</h4><p>经过简单的查询后我们同样也可以在select后加入更多的条件对表进行查询,下面可以用函数来查找我们想要的内容。</p>
<ol>
<li><p>用聚合函数count()计算出表内有多少条行数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(*) from bigdata_user;//用聚合函数count()计算出表内有多少条行数据 </span><br></pre></td></tr></table></figure>
<p>我们可以看到，得出的结果为OK下的那个数字300000（因为我们的small_user.csv中包含了300000条记录，导入到Hive中）。</p>
<p><img src="image-20230521132650584.png" alt="image-20230521132650584"></p>
</li>
<li><p>在函数内部加上distinct，查出uid不重复的数据有多少条</p>
<p>下面继续执行操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(distinct uid) from bigdata_user;//在函数内部加上distinct，查出uid不重复的数据有多少条</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p><img src="image-20230521132722267.png" alt="image-20230521132722267"></p>
</li>
<li><p>查询不重复的数据有多少条(为了排除客户刷单情况) </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select count(*) from (select uid,item_id,behavior_type,item_category,visit_date,province from bigdata_user group by uid,item_id,behavior_type,item_category,visit_date,province having count(*)=1)a;</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p><img src="image-20230521132815436.png" alt="image-20230521132815436"></p>
<p>可以看出，排除掉重复信息以后，只有284444条记录。<br>注意：嵌套语句最好取别名，就是上面的a，否则很容易出现如下错误.</p>
<p><img src="image-20230521131151504.png" alt="image-20230521131151504"></p>
</li>
</ol>
<h4 id="4-关键字条件查询分析"><a href="#4-关键字条件查询分析" class="headerlink" title="4.关键字条件查询分析"></a>4.关键字条件查询分析</h4><ol>
<li><p>以关键字的存在区间为条件的查询<br>使用where可以缩小查询分析的范围和精确度，下面用实例来测试一下。<br>(1)查询2014年12月10日到2014年12月13日有多少人浏览了商品</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select count(*) from bigdata_user where behavior_type=&#x27;1&#x27; and visit_date&lt;&#x27;2014-12-13&#x27; and visit_date&gt;&#x27;2014-12-10&#x27;;</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p><img src="image-20230521132933625.png" alt="image-20230521132933625"></p>
<p>（2）以月的第n天为统计单位，依次显示第n天网站卖出去的商品的个数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(distinct uid), day(visit_date) from bigdata_user where behavior_type=&#x27;4&#x27; group by day(visit_date);</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p><img src="image-20230521133024102.png" alt="image-20230521133024102"><img src="image-20230521133101192.png" alt="image-20230521133101192"></p>
</li>
<li><p>关键字赋予给定值为条件，对其他数据进行分析</p>
<p>取给定时间和给定地点，求当天发出到该地点的货物的数量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.hive&gt; select count(*) from bigdata_user where province=&#x27;江西&#x27; and visit_date=&#x27;2014-12-12&#x27; and behavior_type=&#x27;4&#x27;;</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p><img src="image-20230521133133244.png" alt="image-20230521133133244"></p>
</li>
</ol>
<h4 id="5-根据用户行为分析"><a href="#5-根据用户行为分析" class="headerlink" title="5.根据用户行为分析"></a>5.根据用户行为分析</h4><ol>
<li><p>查询一件商品在某天的购买比例或浏览比例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(*) from bigdata_user where visit_date=&#x27;2014-12-11&#x27;and behavior_type=&#x27;4&#x27;;//查询有多少用户在2014-12-11购买了商品</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(*) from bigdata_user where visit_date =&#x27;2014-12-11&#x27;;//查询有多少用户在2014-12-11点击了该店</span><br></pre></td></tr></table></figure>
<p>根据上面语句得到购买数量和点击数量，两个数相除即可得出当天该商品的购买率。</p>
<p><img src="image-20230521133222377.png" alt="image-20230521133222377"></p>
</li>
<li><p>查询某个用户在某一天点击网站占该天所有点击行为的比例（点击行为包括浏览，加入购物车，收藏，购买）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.hive&gt; select count(*) from bigdata_user where uid=10001082 and visit_date=&#x27;2014-12-12&#x27;;//查询用户10001082在2014-12-12点击网站的次数</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.hive&gt; select count(*) from bigdata_user where visit_date=&#x27;2014-12-12&#x27;;//查询所有用户在这一天点击该网站的次数</span><br></pre></td></tr></table></figure>
<p>上面两条语句的结果相除，就得到了要要求的比例。</p>
<p><img src="image-20230521133330744.png" alt="image-20230521133330744"></p>
</li>
<li><p>给定购买商品的数量范围，查询某一天在该网站的购买该数量商品的用户id</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select uid from bigdata_user where behavior_type=&#x27;4&#x27; and visit_date=&#x27;2014-12-12&#x27; group by uid having count(behavior_type=&#x27;4&#x27;)&gt;5;//查询某一天在该网站购买商品超过5次的用户id</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="image-20230521133404842.png" alt="image-20230521133404842"></p>
<h4 id="6-用户实时查询分析"><a href="#6-用户实时查询分析" class="headerlink" title="6.用户实时查询分析"></a>6.用户实时查询分析</h4><p>某个地区的用户当天浏览网站的次数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.hive&gt; create table scan(province STRING,scan INT) COMMENT &#x27;This is the search of bigdataday&#x27; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; STORED AS TEXTFILE;//创建新的数据表进行存储</span><br><span class="line">2.hive&gt; insert overwrite table scan select province,count(behavior_type) from bigdata_user where behavior_type=&#x27;1&#x27; group by province;//导入数据</span><br><span class="line">3.hive&gt; select * from scan;//显示结果</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<p><img src="image-20230521133528636.png" alt="image-20230521133528636"><img src="image-20230521133546198.png" alt="image-20230521133546198"></p>
<h3 id="三-Hive-MYSQL-HBase数据互导"><a href="#三-Hive-MYSQL-HBase数据互导" class="headerlink" title="三.Hive,MYSQL,HBase数据互导"></a>三.Hive,MYSQL,HBase数据互导</h3><h4 id="1-Hive准备工作"><a href="#1-Hive准备工作" class="headerlink" title="1.Hive准备工作"></a>1.Hive准备工作</h4><p>本教程需要安装Hive、MySQL、HBase和Sqoop。在前面的第一个步骤中，我们在安装Hive的时候就已经一起安装了MySQL（因为我们采用MySQL来存储Hive的元数据），所以，现在你只需要再安装HBase和Sqoop。<br>（1）完成HBase的安装。本教程把HBase安装在了“/usr/local/hbase”目录下，采用伪分布式配置，也就是HBase会使用HDFS来存储数据。<br>（2）完成Sqoop的安装。本教程下载的是sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz，安装目录是“/usr/local/sqoop”。虽然这个sqoop是为hadoop2.0.4版本开发的，本教程Hadoop版本是2.7.1，但是，依然可以顺利使用。</p>
<h4 id="2-Hive预操作"><a href="#2-Hive预操作" class="headerlink" title="2.Hive预操作"></a>2.Hive预操作</h4><p>请登录Linux系统（本教程统一采用hadoop用户名登录系统），然后，打开一个终端（可以按快捷键Ctrl+Alt+T）。<br>本教程中，Hadoop的安装目录是“/usr/local/hadoop”，Hive的安装目录是“/usr/local/hive”。<br>因为需要借助于MySQL保存Hive的元数据，所以，请首先启动MySQL数据库，请在终端中输入下面命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql start  //可以在Linux的任何目录下执行该命令</span><br></pre></td></tr></table></figure>
<p>由于Hive是基于Hadoop的数据仓库，使用HiveQL语言撰写的查询语句，最终都会被Hive自动解析成MapReduce任务由Hadoop去具体执行，因此，需要启动Hadoop，然后再启动Hive。</p>
<p>请执行下面命令启动Hadoop（如果你已经启动了Hadoop就不用再次启动了）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hadoop</span><br><span class="line">2../sbin/start-all.sh</span><br><span class="line">3.jps          #查看一下进程</span><br></pre></td></tr></table></figure>
<p>如果出现下面这些进程，说明Hadoop启动成功了：</p>
<p><img src="image-20230521132143873.png" alt="image-20230521132143873"></p>
<p>下面，继续执行下面命令启动进入Hive：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hive</span><br><span class="line">2../bin/hive   //启动Hive</span><br></pre></td></tr></table></figure>
<p>通过上述过程，我们就完成了MySQL、Hadoop和Hive三者的启动。<br>启动成功以后，就进入了“hive&gt;”命令提示符状态，可以输入类似SQL语句的HiveQL语句。</p>
<p><img src="image-20230521132230389.png" alt="image-20230521132230389"><img src="image-20230521132258269.png" alt="image-20230521132258269"></p>
<p>然后，在“hive&gt;”命令提示符状态下执行下面命令：</p>
<ol>
<li><p>创建临时表user_action</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table dblab.user_action(id STRING,uid STRING, item_id STRING, behavior_type STRING, item_category STRING, visit_date DATE, province STRING) COMMENT &#x27;Welcome to XMU dblab! &#x27; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; STORED AS TEXTFILE;</span><br></pre></td></tr></table></figure>
<p>这个命令执行完以后，Hive会自动在HDFS文件系统中创建对应的数据文件“/user/hive/warehouse/dblab.db/user_action”。<br>我们可以新建一个终端，执行命令查看一下，确认这个数据文件在HDFS中确实被创建了，请在新建的终端中执行下面命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hadoop</span><br><span class="line">2../bin/hdfs dfs -ls /user/hive/warehouse/dblab.db/user_action</span><br></pre></td></tr></table></figure>
<p>可以看到如下结果：</p>
<p><img src="image-20230521141255528.png" alt="image-20230521141255528"><img src="image-20230521143911691.png" alt="image-20230521143911691"></p>
<p>这说明，这个数据文件在HDFS中确实被创建了。注意，这个HDFS中的数据文件，在我们后面的“使用HBase Java API把数据从本地导入到HBase中”操作中会使用到。</p>
<ol>
<li><p>将bigdata_user表中的数据插入到user_action(执行时间：10秒左右)<br>在第二个步骤中，我们已经在Hive中的dblab数据库中创建了一个外部表bigdata_user。下面把dblab.bigdata_user数据插入到dblab.user_action表中，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.hive&gt; INSERT OVERWRITE TABLE dblab.user_action select * from dblab.bigdata_user;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230521144334234.png" alt="image-20230521144334234"></p>
<p>请执行下面命令查询上面的插入命令是否成功执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.hive&gt;select * from user_action limit 10;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230521145123253.png" alt="image-20230521145123253"></p>
</li>
</ol>
</li>
</ol>
<h4 id="3-使用Sqoop将数据从Hive导入MYSQL"><a href="#3-使用Sqoop将数据从Hive导入MYSQL" class="headerlink" title="3.使用Sqoop将数据从Hive导入MYSQL"></a>3.使用Sqoop将数据从Hive导入MYSQL</h4><ol>
<li><p>启动Hadoop集群、MySQL服务<br>前面我们已经启动了Hadoop集群和MySQL服务。这里请确认已经按照前面操作启动成功。</p>
</li>
<li><p>将前面生成的临时表数据从Hive导入到 MySQL 中，包含如下四个步骤。<br>(1)登录 MySQL<br>请在Linux系统中新建一个终端，执行下面命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mysql –u root –p </span><br></pre></td></tr></table></figure>
<p>为了简化操作，本教程直接使用root用户登录MySQL数据库，但是，在实际应用中，建议在MySQL中再另外创建一个用户。<br>执行上面命令以后，就进入了“mysql&gt;”命令提示符状态。<br>(2)创建数据库</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show databases; #显示所有数据库</span><br><span class="line">mysql&gt; create database dblab; #创建dblab数据库</span><br><span class="line">mysql&gt; use dblab; #使用数据库</span><br></pre></td></tr></table></figure>
<p><img src="image-20230521161044837.png" alt="image-20230521161044837"></p>
<p>注意：请使用下面命令查看数据库的编码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;show variables like &quot;char%&quot;;</span><br></pre></td></tr></table></figure>
<p>会显示类似下面的结果：</p>
<p><img src="image-20230521161104860.png" alt="image-20230521161104860"></p>
<p>(3)创建表<br>下面在MySQL的数据库dblab中创建一个新表user_action，并设置其编码为utf-8：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE `dblab`.`user_action` (`id` varchar(50),`uid` varchar(50),`item_id` varchar(50),`behavior_type` varchar(10),`item_category` varchar(50), `visit_date` DATE,`province` varchar(20)) ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="image-20230522124332073.png" alt="image-20230522124332073">   </p>
<p>创建成功后，输入下面命令退出MySQL：</p>
   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; exit;</span><br></pre></td></tr></table></figure>
<p>   (4)导入数据(执行时间：20秒左右)<br>   注意，刚才已经退出MySQL，回到了Shell命令提示符状态。下面就可以执行数据导入操作，</p>
   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. cd /usr/local/sqoop</span><br><span class="line">2. ./bin/sqoop export --connect jdbc:mysql://localhost:3306/dblab --username root --password hadoop --table user_action --export-dir &#x27;/user/hive/warehouse/dblab.db/user_action&#x27; --fields-terminated-by &#x27;\t&#x27;; #导入命令</span><br></pre></td></tr></table></figure>
<p> <img src="image-20230522124620428.png" alt="image-20230522124620428"></p>
<ol>
<li><p>查看MySQL中user_action表数据</p>
<p>下面需要再次启动MySQL，进入“mysql&gt;”命令提示符状态：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mysql –u root –p </span><br></pre></td></tr></table></figure>
<p>然后执行下面命令查询user_action表中的数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.mysql&gt; use dblab;</span><br><span class="line">2.mysql&gt; select * from user_action limit 10;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230522124844454.png" alt="image-20230522124844454"></p>
<p>从Hive导入数据到MySQL中，成功！</p>
</li>
</ol>
<h4 id="4-使用Sqoop将数据从MySQL导入HBase"><a href="#4-使用Sqoop将数据从MySQL导入HBase" class="headerlink" title="4.使用Sqoop将数据从MySQL导入HBase"></a>4.使用Sqoop将数据从MySQL导入HBase</h4><ol>
<li>启动Hadoop集群、MySQL服务、HBase服务<br>之前我们已经启动了Hadoop集群、MySQL服务，这里请确认已经按照前面操作启动成功。这里我们再启动HBase服务。本教程中，HBase的安装目录是“/usr/local/hbase”，而且本教程中，HBase配置为使用HDFS存储数据。<br>请新建一个终端，执行下面命令：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hbase</span><br><span class="line">2../bin/start-hbase.sh</span><br></pre></td></tr></table></figure>
<ol>
<li>启动HBase shell</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hbase</span><br><span class="line">2../bin/hbase shell</span><br></pre></td></tr></table></figure>
<p>启动成功后，就进入了“hbase&gt;”命令提示符状态。</p>
<p><img src="image-20230522125118975.png" alt="image-20230522125106245"></p>
<ol>
<li>创建表user_action</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.hbase&gt; create &#x27;user_action&#x27;, &#123; NAME =&gt; &#x27;f1&#x27;, VERSIONS =&gt; 5&#125;</span><br></pre></td></tr></table></figure>
<p>上面命令在HBase中创建了一个user_action表，这个表中有一个列族f1（你愿意把列族名称取为其他名称也可以，比如列族名称为userinfo），历史版本保留数量为5。</p>
<p><img src="image-20230522125249459.png" alt="image-20230522125249459"></p>
<ol>
<li>导入数据(执行时间：30秒左右)<br>下面新建一个终端，执行下面命令导入数据：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/sqoop</span><br><span class="line">2../bin/sqoop  import  --connect jdbc:mysql://localhost:3306/dblab --username root --password root --table user_action --hbase-table user_action --column-family f1 --hbase-row-key id --hbase-create-table -m 1     #-m 1：使用单个Mapper任务运行导入操作</span><br></pre></td></tr></table></figure>
<p><img src="image-20230522125434747.png" alt="image-20230522125434747"></p>
<p><img src="image-20230522125410487.png" alt="image-20230522125410487"></p>
<ol>
<li>查看HBase中user_action表数据</li>
</ol>
<p>现在，再次切换到HBase Shell运行的那个终端窗口，在“hbase&gt;”命令提示符下，执行下面命令查询刚才导入的数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.habse&gt; scan &#x27;user_action&#x27;,&#123;LIMIT=&gt;10&#125;  #只查询前面10行</span><br></pre></td></tr></table></figure>
<p><img src="image-20230522125532825.png" alt="image-20230522125532825"><img src="image-20230522125543413.png" alt="image-20230522125543413"></p>
<h4 id="5-使用HBase-Java-API把数据从本地导入到HBase中"><a href="#5-使用HBase-Java-API把数据从本地导入到HBase中" class="headerlink" title="5.使用HBase Java API把数据从本地导入到HBase中"></a>5.使用HBase Java API把数据从本地导入到HBase中</h4><ol>
<li><p>启动Hadoop集群、HBase服务<br>请首先确保启动了Hadoop集群和HBase服务。如果还没有启动，请在Linux系统中打开一个终端。<br>首先，按照下面命令启动Hadoop：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hadoop</span><br><span class="line">2../sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p>然后，按照下面命令启动HBase：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/hbase</span><br><span class="line">2../bin/start-hbase.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>数据准备<br>实际上，我们也可以编写Java程序，直接从HDFS中读取数据加载到HBase。但是，这里我们展示的是如何用JAVA程序把本地数据导入到HBase中。你只要把程序做简单修改，就可以实现从HDFS中读取数据加载到HBase。<br>首先，请将之前的user_action数据从HDFS复制到Linux系统的本地文件系统中，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.cd /usr/local/bigdatacase/dataset</span><br><span class="line">2./usr/local/hadoop/bin/hdfs dfs -get /user/hive/warehouse/dblab.db/user_action .</span><br><span class="line">3. #将HDFS上的user_action数据复制到本地当前目录，注意&#x27;.&#x27;表示当前目录</span><br><span class="line">4.cat ./user_action/* | head -10   #查看前10行数据</span><br><span class="line">5.cat ./user_action/00000* &gt; user_action.output #将00000*文件复制一份重命名为user_action.output，*表示通配符</span><br><span class="line">6.head user_action.output  #查看user_action.output前10行</span><br></pre></td></tr></table></figure>
<p><img src="image-20230522130559410.png" alt="image-20230522130559410"></p>
</li>
<li><p>编写数据导入程序<br>我们这里采用Eclipse编写Java程序实现HBase数据导入功能。关于如何使用Eclipse编写Java程序调用HBase API，</p>
<p>请使用Eclipse编写ImportHBase程序（Java代码在本文最后的附录部分），并打包成可执行jar包，命名为ImportHBase.jar。<br>然后，请在“/usr/local/bigdatacase/”目录下面新建一个hbase子目录，用来存放ImportHBase.jar。</p>
</li>
<li><p>数据导入<br>现在开始执行数据导入操作。<br>使用Java程序将数据从本地导入HBase中，导入前，请先清空user_action表。<br>请在之前已经打开的HBase Shell窗口中（也就是在“hbase&gt;”命令提示符下）执行下面操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase&gt; truncate &#x27;user_action&#x27;</span><br><span class="line">hbase&gt; scan &#x27;user_action&#x27;,&#123;LIMIT=&gt;10&#125;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230522130739191.png" alt="image-20230522130739191"></p>
<p>下面就可以运行hadoop jar命令运行程序：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1./usr/local/hadoop/bin/hadoop jar /usr/local/bigdatacase/hbase/ImportHBase.jar HBaseImportTest /usr/local/bigdatacase/dataset/user_action.output</span><br></pre></td></tr></table></figure>
<p><img src="image-20230522131228298.png" alt="image-20230522131228298"></p>
</li>
<li><p>查看HBase中user_action表数据<br>下面，再次切换到HBase Shell窗口，执行下面命令查询数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">habse&gt; scan &#x27;user_action&#x27;,&#123;LIMIT=&gt;10&#125;  #只查询前面10行</span><br></pre></td></tr></table></figure>
<p><img src="image-20230522131301266.png" alt="image-20230522131301266"><img src="image-20230522131320818.png" alt="image-20230522131320818"></p>
<p>实验顺利结束！</p>
</li>
</ol>
<h3 id="四-利用R进行数据可视化分析"><a href="#四-利用R进行数据可视化分析" class="headerlink" title="四.利用R进行数据可视化分析"></a>四.利用R进行数据可视化分析</h3><h4 id="1-环境"><a href="#1-环境" class="headerlink" title="1.环境"></a>1.环境</h4><p>R语言包的安装方式如下：<br>Ubuntu自带的APT包管理器中的R安装包总是落后于标准版，因此需要添加新的镜像源把APT包管理中的R安装包更新到最新版。<br>请登录Linux系统，打开一个终端，然后执行下面命令（并注意保持网络连通，可以访问互联网，因为安装过程要下载各种安装文件）：<br>利用vim打开/etc/apt/sources.list文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/apt/sources.list</span><br></pre></td></tr></table></figure>
<p><img src="image-20230522133033912.png" alt="image-20230522133033912"></p>
<p>在文件的最后一行添加厦门大学的镜像源：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/CRAN/bin/linux/ubuntu bionic-cran40/</span><br></pre></td></tr></table></figure>
<p><img src="image-20230523200613895.png" alt="image-20230523200613895"></p>
<p>退出vim，更新软件源列表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update &amp;&amp; sudo apt upgrade</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果更新软件源出现由于没有公钥，无法验证下列签名的错误，请执行如下命令</span><br><span class="line">sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9</span><br></pre></td></tr></table></figure>
<p><img src="image-20230522134907922.png" alt="image-20230522134907922"></p>
<p><img src="image-20230523200807914.png" alt="image-20230523200807914"></p>
<h4 id="2-安装R语言"><a href="#2-安装R语言" class="headerlink" title="2.安装R语言"></a>2.安装R语言</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install r-base</span><br></pre></td></tr></table></figure>
<p><img src="image-20230523201233068.png" alt="image-20230523201233068"><img src="image-20230523201617463.png" alt="image-20230523201617463"></p>
<p>会提示“您希望继续执行吗？[Y/n]”，可以直接键盘输入“Y”，就可以顺利安装结束。<br>安装结束后，可以执行下面命令启动R：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">R</span><br></pre></td></tr></table></figure>
<p><img src="image-20230523201705646.png" alt="image-20230523201705646"></p>
<p>启动后，会显示如下信息，并进入“&gt;”命令提示符状态：</p>
<p>“&gt;”就是R的命令提示符，你可以在后面输入R语言命令。<br>可以执行下面命令退出R：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">&gt;</span>q<span class="punctuation">(</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p><img src="image-20230522141408928.png" alt="image-20230522141408928"></p>
<h4 id="3-可视化分析MySQL中的数据"><a href="#3-可视化分析MySQL中的数据" class="headerlink" title="3.可视化分析MySQL中的数据"></a>3.可视化分析MySQL中的数据</h4><h5 id="1-安装依赖库"><a href="#1-安装依赖库" class="headerlink" title="1.安装依赖库"></a>1.安装依赖库</h5><p>修改R的源</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">options()$repos  ## 查看使用install.packages安装时的默认镜像</span><br><span class="line">options()$BioC_mirror ##查看使用bioconductor的默认镜像</span><br><span class="line">options(BioC_mirror=&quot;https://mirrors.ustc.edu.cn/bioc/&quot;) ##指定镜像，这个是中国科技大学镜像</span><br><span class="line">options(&quot;repos&quot; = c(CRAN=&quot;https://mirrors.tuna.tsinghua.edu.cn/CRAN/&quot;)) ##指定install.packages安装镜像，这个是清华镜像</span><br></pre></td></tr></table></figure>
<p><img src="image-20230524103935911.png" alt="image-20230524103935911"></p>
<p>为了完成可视化功能，我们需要为R安装一些依赖库，包括：RMySQL、ggplot2、devtools和recharts。<br>RMySQL是一个提供了访问MySQL数据库的R语言接口程序的R语言依赖库。<br>ggplot2和recharts则是R语言中提供绘图可视化功能的依赖库。<br>请启动R进入R命令提示符状态,执行如下命令安装RMySQL：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span><span class="operator">&gt;</span> install.packages<span class="punctuation">(</span><span class="string">&#x27;RMySQL&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p><img src="image-20230523201900882.png" alt="image-20230522141605217"></p>
<p>上面命令执行后， 屏幕会提示”Would you like to user a personal library instead?(y/n)”等问题，只要遇到提问，都在键盘输入y后回车即可。然后，屏幕会显示“—在此连线阶段时请选用CRAN的镜子—”，并会弹出一个白色背景的竖条形窗口，窗口标题是“HTTPS CRAN mirros”，标题下面列出了很多国家的镜像列表，我们可以选择位于China的镜像，比如，选择“China(Beijing)[https]”，然后点击“ok”按钮，就开始安装了。安装过程需要几分钟（当然，也和当前网络速度有关系）。<br>由于不同用户的Ubuntu开发环境不一样，安装有很大可能因为缺少组件导致失败，如果出现如下错误信息：</p>
<p>只要根据错误给出的错误信息，进行操作即可。q()退出R命令提示符状态，回到Shell状态,笔者的系统是Ubuntu 16.04,那么，根据上面的英文错误信息，就需要在Shell命令提示符状态下执行下面命令安装libmariadb-client-lgpl-dev：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.sudo apt-get install libmariadb-client-lgpl-dev</span><br></pre></td></tr></table></figure>
<p><img src="image-20230523204134560.png" alt="image-20230523204134560"></p>
<p>然后，再次输入下面命令进入R命令提示符状态：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">R</span><br></pre></td></tr></table></figure>
<p>启动后，并进入“&gt;”命令提示符状态。然后，执行如下命令安装绘图包ggplot2，如果还出现缺少组件的错误，请按照上面的解决方案解决！</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.&gt; install.packages(&#x27;ggplot2&#x27;)</span><br></pre></td></tr></table></figure>
<p><img src="image-20230522142215489.png" alt="image-20230522142215489"></p>
<p>然后，屏幕会显示“—在此连线阶段时请选用CRAN的镜子—”，并会弹出一个白色背景的竖条形窗口，窗口标题是“HTTPS CRAN mirros”，标题下面列出了很多国家的镜像列表，我们可以选择位于China的镜像，比如，选择“China(Beijing)[https]”，然后点击“ok”按钮，就开始安装了。这个命令运行后，大概需要安装10分钟时间（当然，也和当前网络速度有关系）。<br>下面继续运行下面命令安装devtools：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.&gt; install.packages(&#x27;devtools&#x27;)</span><br></pre></td></tr></table></figure>
<p><img src="image-20230522142345991.png" alt="image-20230522142345991"></p>
<p>如果在上面安装devtools的过程中，又出现了错误，处理方法很简单，还是按照上面介绍的方法，根据屏幕上给出的英文错误信息，缺少什么软件，就用sudo apt-get install命令安装该软件就可以了。笔者在Ubuntu16.04上执行devtools安装时，出现了三次错误，笔者根据每次错误的英文提示信息，安装了三个软件libssl-dev、libssh2-1-dev、libcurl4-openssl-dev，安装命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.sudo apt-get install libssl-dev</span><br><span class="line">2.sudo apt-get install libssh2-1-dev</span><br><span class="line">3.sudo apt-get install libcurl4-openssl-dev</span><br></pre></td></tr></table></figure>
<p><img src="image-20230522142709011.png" alt="image-20230522142709011"></p>
<p><img src="image-20230522142728262.png" alt="image-20230522142728262"></p>
<p>读者在安装过程中，可能会出现不同的错误，按照同样的处理方法可以顺利解决。</p>
<p>下面在R命令提示符下再执行如下命令安装taiyun/recharts：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.&gt; devtools::install_github(&#x27;taiyun/recharts&#x27;)</span><br></pre></td></tr></table></figure>
<h5 id="2-分析"><a href="#2-分析" class="headerlink" title="2.分析"></a>2.分析</h5><p>以下分析使用的函数方法，都可以使用如下命令查询函数的相关文档。例如：查询sort()函数如何使用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.?sort</span><br></pre></td></tr></table></figure>
<p>这时，就会进入冒号“:”提示符状态（也就是帮助文档状态），在冒号后面输入q即可退出帮助文档状态，返回到R提示符状态！</p>
<ol>
<li><p>连接MySQL,并获取数据</p>
<p>请在Linux系统中新建另外一个终端，然后执行下面命令启动MySQL数据库：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.service mysql start</span><br></pre></td></tr></table></figure>
<p>下面，让我们查看一下MySQL数据库中的数据，请执行下面命令进入MySQL命令提示符状态：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.mysql -u root -p</span><br></pre></td></tr></table></figure>
<p>下面就可以输入一些SQL语句查询数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.mysql&gt; use dblab;</span><br><span class="line">2.mysql&gt; select * from user_action limit 10;</span><br></pre></td></tr></table></figure>
<p><img src="image-20230522200233331.png" alt="image-20230522200233331"></p>
<p>然后切换到刚才已经打开的R命令提示符终端窗口：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>library<span class="punctuation">(</span>RMySQL<span class="punctuation">)</span></span><br><span class="line"><span class="number">2.</span>conn <span class="operator">&lt;-</span> dbConnect<span class="punctuation">(</span>MySQL<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">,</span>dbname<span class="operator">=</span><span class="string">&#x27;dblab&#x27;</span><span class="punctuation">,</span>username<span class="operator">=</span><span class="string">&#x27;root&#x27;</span><span class="punctuation">,</span>password<span class="operator">=</span><span class="string">&#x27;hadoop&#x27;</span><span class="punctuation">,</span>host<span class="operator">=</span><span class="string">&quot;127.0.0.1&quot;</span><span class="punctuation">,</span>port<span class="operator">=</span><span class="number">3306</span><span class="punctuation">)</span></span><br><span class="line"><span class="number">3.</span>user_action <span class="operator">&lt;-</span> dbGetQuery<span class="punctuation">(</span>conn<span class="punctuation">,</span><span class="string">&#x27;select * from user_action&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p><img src="image-20230522200239746.png" alt="image-20230522200239746"></p>
<ol>
<li><p>分析消费者对商品的行为</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.</span>summary<span class="punctuation">(</span>user_action<span class="operator">$</span>behavior_type<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p><img src="image-20230522200248577.png" alt="image-20230522200248577"></p>
<p>summary() 函数可以得到样本数据类型和长度,如果样本是数值型,我们还能得到样本数据的最小值、最大值、四分位数以及均值信息。（包括计数、最小值、第一四分位数、中位数、第三四分位数和最大值）<br>得到结果：</p>
<p>可以看出原来的MySQL数据中,消费者行为变量的类型是字符型。这样不好做比较，需要把消费者行为变量转换为数值型变量</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>summary<span class="punctuation">(</span><span class="built_in">as.numeric</span><span class="punctuation">(</span>user_action<span class="operator">$</span>behavior_type<span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>得到结果：</p>
<p><img src="image-20230522200255218.png" alt="image-20230522200255218"></p>
<p>接下来用柱状图表示：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2.</span>library<span class="punctuation">(</span>ggplot2<span class="punctuation">)</span></span><br><span class="line"><span class="number">3.</span>ggplot<span class="punctuation">(</span>user_action<span class="punctuation">,</span>aes<span class="punctuation">(</span><span class="built_in">as.numeric</span><span class="punctuation">(</span>behavior_type<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="operator">+</span>geom_histogram<span class="punctuation">(</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>在使用ggplot2库的时候，需要使用library导入库。ggplot()绘制时，创建绘图对象，即第一个图层，包含两个参数(数据与变量名称映射).变量名称需要被包含aes函数里面。ggplot2的图层与图层之间用“+”进行连接。ggplot2包中的geom_histogram()可以很方便的实现直方图的绘制，<code>aes()</code>函数指定了变量的映射方式将数值型<code>behavior_type</code>列映射到x轴上。</p>
<p>分析结果如下图:</p>
<p><img src="image-20230628190044620.png" alt="image-20230628190044620"></p>
<p>从上图可以得到:大部分消费者行为仅仅只是浏览。只有很少部分的消费者会购买商品。</p>
</li>
<li><p>分析哪一类商品被购买总量前十的商品和被购买总量</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.</span>temp <span class="operator">&lt;-</span> subset<span class="punctuation">(</span>user_action<span class="punctuation">,</span><span class="built_in">as.numeric</span><span class="punctuation">(</span>behavior_type<span class="punctuation">)</span><span class="operator">==</span><span class="number">4</span><span class="punctuation">)</span> <span class="comment"># 获取子数据集</span></span><br><span class="line"><span class="number">1.</span>count <span class="operator">&lt;-</span> sort<span class="punctuation">(</span>table<span class="punctuation">(</span>temp<span class="operator">$</span>item_category<span class="punctuation">)</span><span class="punctuation">,</span>decreasing <span class="operator">=</span> <span class="built_in">T</span><span class="punctuation">)</span> <span class="comment">#排序</span></span><br><span class="line"><span class="number">2.</span>print<span class="punctuation">(</span>count<span class="punctuation">[</span><span class="number">1</span><span class="operator">:</span><span class="number">10</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># 获取第1到10个排序结果</span></span><br></pre></td></tr></table></figure>
<p>subset()函数，从某一个数据框中选择出符合某条件的数据或是相关的列.table()对应的就是统计学中的列联表，是一种记录频数的方法.sort()进行排序，返回排序后的数值向量。</p>
<p>在<code>user_action</code>数据框中创建了一个子集<code>temp</code>，该子集包含在<code>behavior_type</code>列中数值等于4的所有行。</p>
<p>代码使用了<code>table()</code>函数对不同<code>item_category</code>的出现次数进行计数，然后使用<code>sort()</code>函数将计数结果按照频次从大到小进行排序。在<code>sort()</code>函数中，<code>decreasing=T</code>参数表示按照逆序排列</p>
<p>得到结果：</p>
<p><img src="image-20230522200312006.png" alt="image-20230522200312006"></p>
<p>结果第一行表示商品分类,该类下被消费的数次。<br>接下来用散点图表示:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3.</span>result <span class="operator">&lt;-</span> as.data.frame<span class="punctuation">(</span>count<span class="punctuation">[</span><span class="number">1</span><span class="operator">:</span><span class="number">10</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment">#将count矩阵结果转换成数据框</span></span><br><span class="line"><span class="number">4.</span>ggplot<span class="punctuation">(</span>result<span class="punctuation">,</span>aes<span class="punctuation">(</span>Var1<span class="punctuation">,</span>Freq<span class="punctuation">,</span>col<span class="operator">=</span>factor<span class="punctuation">(</span>Var1<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="operator">+</span>geom_point<span class="punctuation">(</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>通过 as.data.frame() 把矩阵等转换成为数据框.<br>分析结果如下图:</p>
<p><img src="image-20230522200318462.png" alt="image-20230522200318462"></p>
</li>
<li><p>分析每年的哪个月份购买商品的量最多</p>
<p>从MySQL直接获取的数据中visit_date变量都是2014年份,并没有划分出具体的月份,那么可以在数据集增加一列月份数据。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.</span>month <span class="operator">&lt;-</span> substr<span class="punctuation">(</span>user_action<span class="operator">$</span>visit_date<span class="punctuation">,</span><span class="number">6</span><span class="punctuation">,</span><span class="number">7</span><span class="punctuation">)</span> <span class="comment"># visit_date变量中截取月份</span></span><br><span class="line"><span class="number">1.</span>user_action <span class="operator">&lt;-</span> cbind<span class="punctuation">(</span>user_action<span class="punctuation">,</span>month<span class="punctuation">)</span> <span class="comment"># user_action增加一列月份数据</span></span><br></pre></td></tr></table></figure>
<p>接下来用柱状图分别表示消费者购买量</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2.</span>ggplot<span class="punctuation">(</span>user_action<span class="punctuation">,</span>aes<span class="punctuation">(</span><span class="built_in">as.numeric</span><span class="punctuation">(</span>behavior_type<span class="punctuation">)</span><span class="punctuation">,</span>col<span class="operator">=</span>factor<span class="punctuation">(</span>month<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="operator">+</span>geom_histogram<span class="punctuation">(</span><span class="punctuation">)</span><span class="operator">+</span>facet_grid<span class="punctuation">(</span>.<span class="operator">~</span>month<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>aes()函数中的col属性可以用来设置颜色。factor()函数则是把数值变量转换成分类变量,作用是以不同的颜色表示。如果不使用factor()函数，颜色将以同一种颜色渐变的颜色表现。 facet_grid(.~month)表示柱状图按照不同月份进行分区。<br>由于MySQL获取的数据中只有11月份和12月份的数据,所以上图只有显示两个表格。<br>分析结果如下图：</p>
<p><img src="image-20230628190435063.png" alt="image-20230628190435063"><img src="image-20230628190702238.png" alt="image-20230628190702238"></p>
</li>
<li><p>分析国内哪个省份的消费者最有购买欲望</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.</span>library<span class="punctuation">(</span>recharts<span class="punctuation">)</span></span><br><span class="line"><span class="number">1.</span>rel <span class="operator">&lt;-</span> as.data.frame<span class="punctuation">(</span>table<span class="punctuation">(</span>temp<span class="operator">$</span>province<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="number">2.</span>provinces <span class="operator">&lt;-</span> rel<span class="operator">$</span>Var1</span><br><span class="line"><span class="number">3.</span>x <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="number">4.</span><span class="keyword">for</span><span class="punctuation">(</span>n <span class="keyword">in</span> provinces<span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line"><span class="number">5.</span>x<span class="punctuation">[</span><span class="built_in">length</span><span class="punctuation">(</span>x<span class="punctuation">)</span><span class="operator">+</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">=</span> nrow<span class="punctuation">(</span>subset<span class="punctuation">(</span>temp<span class="punctuation">,</span><span class="punctuation">(</span>province<span class="operator">==</span>n<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="number">6.</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="number">7.</span>mapData <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>province<span class="operator">=</span>rel<span class="operator">$</span>Var1<span class="punctuation">,</span></span><br><span class="line"><span class="number">8.</span>count<span class="operator">=</span>x<span class="punctuation">,</span> stringsAsFactors<span class="operator">=</span><span class="built_in">F</span><span class="punctuation">)</span> <span class="comment"># 设置地图信息</span></span><br><span class="line"><span class="number">9.</span>eMap<span class="punctuation">(</span>mapData<span class="punctuation">,</span> namevar<span class="operator">=</span><span class="operator">~</span>province<span class="punctuation">,</span> datavar <span class="operator">=</span> <span class="operator">~</span>count<span class="punctuation">)</span> <span class="comment">#画出中国地图</span></span><br></pre></td></tr></table></figure>
<p>nrow()用来计算数据集的行数。<br>分析结果如下图:</p>
<p><img src="image-20230522200334675.png" alt="image-20230522200334675"></p>
</li>
</ol>
</li>
</ol>
<p>大数据案例网站用户购物行为分析所有实验步骤到此结束！</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://hanmengting.github.io">韩梦婷</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://hanmengting.github.io/2023/10/22/%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%EF%BC%88%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AF%BE%E8%AE%BE%EF%BC%89/">https://hanmengting.github.io/2023/10/22/%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%EF%BC%88%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AF%BE%E8%AE%BE%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://hanmengting.github.io" target="_blank">Hik🐣</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="http://img.netbian.com/file/2024/0310/161609O75fK.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/10/22/js%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%82%B9/" title="js基本要点"><img class="cover" src="http://img.netbian.com/file/2023/0926/234201vj2AV.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">js基本要点</div></div></a></div><div class="next-post pull-right"><a href="/2023/10/22/KVM-QEMU%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E5%92%8C%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="KVM/QEMU虚拟化环境和开发环境搭建"><img class="cover" src="http://img.netbian.com/file/2023/1022/210232lVk42.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">KVM/QEMU虚拟化环境和开发环境搭建</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://p.qqan.com/up/2020-12/16086065967661528.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">韩梦婷</div><div class="author-info__description">🟠：双向的奔赴才有意义🍯</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/hanmengting"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="toc-number">1.</span> <span class="toc-text">一. 环境搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89%E5%AE%89%E8%A3%85Linux%E7%B3%BB%E7%BB%9F-ubuntu18-04-6"><span class="toc-number">1.1.</span> <span class="toc-text">（一）安装Linux系统(ubuntu18.04.6)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89%E5%AE%89%E8%A3%85Hadoop"><span class="toc-number">1.2.</span> <span class="toc-text">（二）安装Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAhadoop%E7%94%A8%E6%88%B7"><span class="toc-number">1.2.1.</span> <span class="toc-text">创建hadoop用户</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9B%B4%E6%96%B0apt"><span class="toc-number">1.2.2.</span> <span class="toc-text">更新apt</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85ssh%E3%80%81%E9%85%8D%E7%BD%AEssh%E6%97%A0%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95"><span class="toc-number">1.2.3.</span> <span class="toc-text">安装ssh、配置ssh无密码登录</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Java%E7%8E%AF%E5%A2%83"><span class="toc-number">1.2.4.</span> <span class="toc-text">安装Java环境</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Hadoop2-7-1"><span class="toc-number">1.2.5.</span> <span class="toc-text">安装Hadoop2.7.1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E5%8D%95%E6%9C%BA%E9%85%8D%E7%BD%AE-%E9%9D%9E%E5%88%86%E5%B8%83%E5%BC%8F"><span class="toc-number">1.2.6.</span> <span class="toc-text">Hadoop单机配置(非分布式)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%85%8D%E7%BD%AE"><span class="toc-number">1.2.7.</span> <span class="toc-text">Hadoop伪分布式配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%90%E8%A1%8CHadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E4%BE%8B"><span class="toc-number">1.2.8.</span> <span class="toc-text">运行Hadoop伪分布式实例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89%E5%AE%89%E8%A3%85MYSQL"><span class="toc-number">1.3.</span> <span class="toc-text">（三）安装MYSQL</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E4%BB%A5%E4%B8%8B%E5%91%BD%E4%BB%A4%E5%8D%B3%E5%8F%AF%E8%BF%9B%E8%A1%8Cmysql%E5%AE%89%E8%A3%85%EF%BC%8C%E6%B3%A8%E6%84%8F%E5%AE%89%E8%A3%85%E5%89%8D%E5%85%88%E6%9B%B4%E6%96%B0%E4%B8%80%E4%B8%8B%E8%BD%AF%E4%BB%B6%E6%BA%90%E4%BB%A5%E8%8E%B7%E5%BE%97%E6%9C%80%E6%96%B0%E7%89%88%E6%9C%AC%EF%BC%9A"><span class="toc-number">1.3.1.</span> <span class="toc-text">使用以下命令即可进行mysql安装，注意安装前先更新一下软件源以获得最新版本：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BDmysql-jdbc%E5%8C%85"><span class="toc-number">1.3.2.</span> <span class="toc-text">下载mysql jdbc包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E5%B9%B6%E7%99%BB%E9%99%86mysql-shell"><span class="toc-number">1.3.3.</span> <span class="toc-text">启动并登陆mysql shell</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B0%E5%BB%BAhive%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">1.3.4.</span> <span class="toc-text">新建hive数据库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEmysql%E5%85%81%E8%AE%B8hive%E6%8E%A5%E5%85%A5"><span class="toc-number">1.3.5.</span> <span class="toc-text">配置mysql允许hive接入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8hive"><span class="toc-number">1.3.6.</span> <span class="toc-text">启动hive</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%88%E5%9B%9B%EF%BC%89%E5%AE%89%E8%A3%85HBase"><span class="toc-number">1.4.</span> <span class="toc-text">（四）安装HBase</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#hbase%E5%AE%89%E8%A3%85"><span class="toc-number">1.4.1.</span> <span class="toc-text">hbase安装</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hbase%E9%85%8D%E7%BD%AE"><span class="toc-number">1.4.2.</span> <span class="toc-text">hbase配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E8%BF%90%E8%A1%8CHBase"><span class="toc-number">1.4.3.</span> <span class="toc-text">测试运行HBase</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%88%E4%BA%94%EF%BC%89%E5%AE%89%E8%A3%85Hive"><span class="toc-number">1.5.</span> <span class="toc-text">（五）安装Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E5%B9%B6%E5%8A%A0%E5%8E%8Bhive%E5%AE%89%E8%A3%85%E5%8C%85"><span class="toc-number">1.5.1.</span> <span class="toc-text">下载并加压hive安装包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">1.5.2.</span> <span class="toc-text">配置环境变量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9-usr-local-hive-conf%E4%B8%8B%E7%9A%84hive-site-xml"><span class="toc-number">1.5.3.</span> <span class="toc-text">修改&#x2F;usr&#x2F;local&#x2F;hive&#x2F;conf下的hive-site.xml</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E5%AE%89%E8%A3%85%E5%AE%8Cmysql%E5%B9%B6%E9%85%8D%E7%BD%AE%E5%AE%8C%E6%88%90%E4%B9%8B%E5%90%8E%E5%90%AF%E5%8A%A8hive"><span class="toc-number">1.5.4.</span> <span class="toc-text">在安装完mysql并配置完成之后启动hive</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%88%E5%85%AD%EF%BC%89%E5%AE%89%E8%A3%85Eclipse"><span class="toc-number">1.6.</span> <span class="toc-text">（六）安装Eclipse</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%88%E4%B8%83%EF%BC%89%E5%AE%89%E8%A3%85sqoop"><span class="toc-number">1.7.</span> <span class="toc-text">（七）安装sqoop</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E5%B9%B6%E8%A7%A3%E5%8E%8Bsqoop1-4-6"><span class="toc-number">1.7.1.</span> <span class="toc-text">下载并解压sqoop1.4.6</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6sqoop-env-sh"><span class="toc-number">1.7.2.</span> <span class="toc-text">修改配置文件sqoop-env.sh</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F-1"><span class="toc-number">1.7.3.</span> <span class="toc-text">配置环境变量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86mysql%E9%A9%B1%E5%8A%A8%E5%8C%85%E6%8B%B7%E8%B4%9D%E5%88%B0-SQOOP-HOME-lib"><span class="toc-number">1.7.4.</span> <span class="toc-text">将mysql驱动包拷贝到$SQOOP_HOME&#x2F;lib</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E4%B8%8EMySQL%E7%9A%84%E8%BF%9E%E6%8E%A5"><span class="toc-number">1.7.5.</span> <span class="toc-text">测试与MySQL的连接</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90"><span class="toc-number">2.</span> <span class="toc-text">二.网站用户行为分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80-%E6%9C%AC%E5%9C%B0%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E4%BC%A0%E5%88%B0%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive"><span class="toc-number">2.1.</span> <span class="toc-text">一. 本地数据集上传到数据仓库Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E4%B8%8B%E8%BD%BD"><span class="toc-number">2.1.1.</span> <span class="toc-text">1. 实验数据集的下载</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">2.1.2.</span> <span class="toc-text">2.  数据集的预处理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C-Hive%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="toc-number">2.2.</span> <span class="toc-text">二. Hive数据分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%93%8D%E4%BD%9CHive"><span class="toc-number">2.2.1.</span> <span class="toc-text">1.操作Hive</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%AE%80%E5%8D%95%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.简单查询分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%9F%A5%E8%AF%A2%E6%9D%A1%E6%95%B0%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90"><span class="toc-number">2.2.3.</span> <span class="toc-text">3.查询条数统计分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%85%B3%E9%94%AE%E5%AD%97%E6%9D%A1%E4%BB%B6%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90"><span class="toc-number">2.2.4.</span> <span class="toc-text">4.关键字条件查询分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E6%A0%B9%E6%8D%AE%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90"><span class="toc-number">2.2.5.</span> <span class="toc-text">5.根据用户行为分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E7%94%A8%E6%88%B7%E5%AE%9E%E6%97%B6%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90"><span class="toc-number">2.2.6.</span> <span class="toc-text">6.用户实时查询分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89-Hive-MYSQL-HBase%E6%95%B0%E6%8D%AE%E4%BA%92%E5%AF%BC"><span class="toc-number">2.3.</span> <span class="toc-text">三.Hive,MYSQL,HBase数据互导</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Hive%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-number">2.3.1.</span> <span class="toc-text">1.Hive准备工作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Hive%E9%A2%84%E6%93%8D%E4%BD%9C"><span class="toc-number">2.3.2.</span> <span class="toc-text">2.Hive预操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E4%BD%BF%E7%94%A8Sqoop%E5%B0%86%E6%95%B0%E6%8D%AE%E4%BB%8EHive%E5%AF%BC%E5%85%A5MYSQL"><span class="toc-number">2.3.3.</span> <span class="toc-text">3.使用Sqoop将数据从Hive导入MYSQL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E4%BD%BF%E7%94%A8Sqoop%E5%B0%86%E6%95%B0%E6%8D%AE%E4%BB%8EMySQL%E5%AF%BC%E5%85%A5HBase"><span class="toc-number">2.3.4.</span> <span class="toc-text">4.使用Sqoop将数据从MySQL导入HBase</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E4%BD%BF%E7%94%A8HBase-Java-API%E6%8A%8A%E6%95%B0%E6%8D%AE%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%AF%BC%E5%85%A5%E5%88%B0HBase%E4%B8%AD"><span class="toc-number">2.3.5.</span> <span class="toc-text">5.使用HBase Java API把数据从本地导入到HBase中</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B-%E5%88%A9%E7%94%A8R%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%86%E6%9E%90"><span class="toc-number">2.4.</span> <span class="toc-text">四.利用R进行数据可视化分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%8E%AF%E5%A2%83"><span class="toc-number">2.4.1.</span> <span class="toc-text">1.环境</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%AE%89%E8%A3%85R%E8%AF%AD%E8%A8%80"><span class="toc-number">2.4.2.</span> <span class="toc-text">2.安装R语言</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%86%E6%9E%90MySQL%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="toc-number">2.4.3.</span> <span class="toc-text">3.可视化分析MySQL中的数据</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%BA%93"><span class="toc-number">2.4.3.1.</span> <span class="toc-text">1.安装依赖库</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E5%88%86%E6%9E%90"><span class="toc-number">2.4.3.2.</span> <span class="toc-text">2.分析</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/10/25/HTML%E7%AE%80%E4%BB%8B/" title="HTML简介"><img src="http://img.netbian.com/file/2024/0111/004351ZZdk9.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="HTML简介"/></a><div class="content"><a class="title" href="/2023/10/25/HTML%E7%AE%80%E4%BB%8B/" title="HTML简介">HTML简介</a><time datetime="2023-10-25T11:32:37.000Z" title="发表于 2023-10-25 19:32:37">2023-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/22/KVM-QEMU%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E5%92%8C%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="KVM/QEMU虚拟化环境和开发环境搭建"><img src="http://img.netbian.com/file/2023/1022/210232lVk42.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="KVM/QEMU虚拟化环境和开发环境搭建"/></a><div class="content"><a class="title" href="/2023/10/22/KVM-QEMU%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E5%92%8C%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="KVM/QEMU虚拟化环境和开发环境搭建">KVM/QEMU虚拟化环境和开发环境搭建</a><time datetime="2023-10-22T10:08:19.000Z" title="发表于 2023-10-22 18:08:19">2023-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/22/%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%EF%BC%88%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AF%BE%E8%AE%BE%EF%BC%89/" title="网站用户行为分析（大数据课设）"><img src="http://img.netbian.com/file/2024/0310/161609O75fK.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="网站用户行为分析（大数据课设）"/></a><div class="content"><a class="title" href="/2023/10/22/%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90%EF%BC%88%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AF%BE%E8%AE%BE%EF%BC%89/" title="网站用户行为分析（大数据课设）">网站用户行为分析（大数据课设）</a><time datetime="2023-10-22T10:05:55.000Z" title="发表于 2023-10-22 18:05:55">2023-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/22/js%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%82%B9/" title="js基本要点"><img src="http://img.netbian.com/file/2023/0926/234201vj2AV.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="js基本要点"/></a><div class="content"><a class="title" href="/2023/10/22/js%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%82%B9/" title="js基本要点">js基本要点</a><time datetime="2023-10-22T09:57:06.000Z" title="发表于 2023-10-22 17:57:06">2023-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/22/css%E7%AE%80%E4%BB%8B/" title="css简介"><img src="http://img.netbian.com/file/2024/0221/100206JM5bs.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="css简介"/></a><div class="content"><a class="title" href="/2023/10/22/css%E7%AE%80%E4%BB%8B/" title="css简介">css简介</a><time datetime="2023-10-22T09:24:12.000Z" title="发表于 2023-10-22 17:24:12">2023-10-22</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('http://img.netbian.com/file/2024/0310/161609O75fK.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By 韩梦婷</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>